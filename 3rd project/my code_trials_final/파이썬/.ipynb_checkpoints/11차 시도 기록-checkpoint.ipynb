{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8594c368",
   "metadata": {},
   "source": [
    "### 현재까지의 기록\n",
    "- 음성(CNN)과 텍스트(KoELECTRA)로 멀티모달 앙상블 모델을 만든 결과 약 70% 전후의 시험 정확도가 나옴\n",
    "    - 음성모델, 텍스트 모델, 스케일러, 관련 파일 : model_11-0.4734.keras, 7th_koElectraModel.pt, rscaler.pkl, modelCombiner\n",
    "- 성능에 비해 실행 속도가 지나치게 느린 이슈 발생\n",
    "- 실행 속도를 늘리기 위해 모델 변경 시험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3958223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "# import librosa.display\n",
    "# import IPython.display as ipd\n",
    "# from IPython.display import Audio\n",
    "\n",
    "# from random import randint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import pydub #오디오 파일을 다루는 라이브러리 : 변환 조작 재생 분석 등\n",
    "# import gTTS # 텍스트를 음성으로 변환하는 라이브러리. 나는 당장 쓸 일 없을 듯?\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "# import np_utils\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "import nlpaug.augmenter.audio as naa\n",
    "# import wave\n",
    "\n",
    "import glob\n",
    "import os\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "# os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "# import sys\n",
    "# import logging\n",
    "# import unicodedata\n",
    "# from shutil import copyfile\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"always\")\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, FunctionTransformer\n",
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "# from sklearn.metrics import recall_score,precision_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "# from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "# from scikeras.wrappers import KerasClassifier \n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# from sklearn.metrics import accuracy_score, f1_score\n",
    "# from sklearn.svm import SVC \n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "import joblib\n",
    "from joblib import dump, load\n",
    "# from xgboost import XGBClassifier \n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint,EarlyStopping, Callback\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense, Conv1D, MaxPooling1D, Flatten, Dropout, concatenate\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.utils import to_categorical\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# from keras.metrics import Precision, Recall, F1Score\n",
    "\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# import torch.optim as optim\n",
    "# from torch.optim import AdamW\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# np.bool = np.bool_\n",
    "\n",
    "# import mxnet\n",
    "# import gluonnlp as nlp\n",
    "\n",
    "# from transformers import AdamW\n",
    "# from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "# from transformers import BertModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50d951ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_csv = pd.read_csv('d:/Data2/csv/train.csv')\n",
    "# test_csv = pd.read_csv('d:/Data2/csv/test.csv')\n",
    "train_csv = pd.read_csv('d:/Data2/csv/train_csv_9.csv')\n",
    "test_csv = pd.read_csv('d:/Data2/csv/test_csv_9.csv')\n",
    "exam_list = ['5fbe364744697678c497c07f','5fbe361844697678c497c07e']\n",
    "\n",
    "train_audio_path = 'd:/Data2/train/'\n",
    "test_audio_path = 'd:/Data2/test/'\n",
    "exam_audio_path = 'd:/Data2/exam/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1be9ac",
   "metadata": {},
   "source": [
    "### 정확도를 올리기 위해 비교적 중요도가 낮은 공포, 놀람, 혐오의 세 가지 감정은 빼보기로 함\n",
    "### 음성적 특질 중 일부만을 사용한 경우와 지금처럼 전체를 사용한 경우를 비교하여 더 성능 좋은 것 선택하기로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dc2deb65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Angry', 'Disgust', 'Fear', 'Happiness', 'Neutral', 'Sadness', 'Surprise'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_csv['감정'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e8ba3",
   "metadata": {},
   "source": [
    "```python\n",
    "#파일 중 공포, 놀람, 혐오의 감정인 경우 다른 폴더로 옮기기\n",
    "for i in range(len(train_csv)): #전체 파일의 개수만큼의 회수\n",
    "    if train_csv['감정레이블'][i] == 1 or train_csv['감정레이블'][i] == 2 or train_csv['감정레이블'][i] == 6: \n",
    "        shutil.move(train_audio_path+train_csv['path'][i], f\"d:/Data2/train_disgust,fear,surprise/{train_csv['path'][i]}\")\n",
    "\n",
    "\n",
    "for i in range(len(test_csv)): #전체 파일의 개수만큼의 회수\n",
    "    if test_csv['감정레이블'][i] == 1 or test_csv['감정레이블'][i] == 2 or test_csv['감정레이블'][i] == 6: \n",
    "        shutil.move(test_audio_path+test_csv['path'][i], f\"d:/Data2/test_disgust,fear,surprise/{test_csv['path'][i]}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a869012",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "709\n",
      "6369\n"
     ]
    }
   ],
   "source": [
    "#개수 확인\n",
    "print(len(test_csv[test_csv['감정레이블']==1])+len(test_csv[test_csv['감정레이블']==2])+len(test_csv[test_csv['감정레이블']==6]) )\n",
    "print(len(train_csv[train_csv['감정레이블']==1])+len(train_csv[train_csv['감정레이블']==2])+len(train_csv[train_csv['감정레이블']==6]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8c65ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39588"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "359f9aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wav_id</th>\n",
       "      <th>발화문</th>\n",
       "      <th>감정</th>\n",
       "      <th>감정레이블</th>\n",
       "      <th>Angry</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Happiness</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5e258fd1305bcf3ad153a6a4</td>\n",
       "      <td>어 청소 니가 대신 해 줘!</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5e258fd1305bcf3ad153a6a4.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5e258fe2305bcf3ad153a6a5</td>\n",
       "      <td>둘 다 청소 하기 싫어 귀찮아</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5e258fe2305bcf3ad153a6a5.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5e258ff5305bcf3ad153a6a6</td>\n",
       "      <td>둘 다 하기 싫어서 화내</td>\n",
       "      <td>Angry</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5e258ff5305bcf3ad153a6a6.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5e25902f305bcf3ad153a6a9</td>\n",
       "      <td>그럼 방세는 어떡해</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5e25902f305bcf3ad153a6a9.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5e27ec9f5807b852d9e01542</td>\n",
       "      <td>나 결국 헤어졌어</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5e27ec9f5807b852d9e01542.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39583</th>\n",
       "      <td>5fb8e0834c55eb78bd7cdec2</td>\n",
       "      <td>문이 안 열려</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5fb8e0834c55eb78bd7cdec2.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39584</th>\n",
       "      <td>5f8cd53af8fac448cc0a783a</td>\n",
       "      <td>친구도 만나야지 만나서 실컷 수다도 떨고 맛있는 것도 먹고</td>\n",
       "      <td>Happiness</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5f8cd53af8fac448cc0a783a.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39585</th>\n",
       "      <td>5fbb59c144697678c497b6d6</td>\n",
       "      <td>많이 힘들거다</td>\n",
       "      <td>Disgust</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5fbb59c144697678c497b6d6.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39586</th>\n",
       "      <td>5f0d6624b140144dfcff35c6</td>\n",
       "      <td>엊그제 갑자기 의식을 잃었어 나아지는가 싶더니 결국 죽고 말았지</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5f0d6624b140144dfcff35c6.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39587</th>\n",
       "      <td>5fb88b9844697678c497adf1</td>\n",
       "      <td>한 명이야 한 명!</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5fb88b9844697678c497adf1.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39588 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         wav_id                                  발화문  \\\n",
       "0      5e258fd1305bcf3ad153a6a4                      어 청소 니가 대신 해 줘!   \n",
       "1      5e258fe2305bcf3ad153a6a5                     둘 다 청소 하기 싫어 귀찮아   \n",
       "2      5e258ff5305bcf3ad153a6a6                        둘 다 하기 싫어서 화내   \n",
       "3      5e25902f305bcf3ad153a6a9                           그럼 방세는 어떡해   \n",
       "4      5e27ec9f5807b852d9e01542                            나 결국 헤어졌어   \n",
       "...                         ...                                  ...   \n",
       "39583  5fb8e0834c55eb78bd7cdec2                              문이 안 열려   \n",
       "39584  5f8cd53af8fac448cc0a783a     친구도 만나야지 만나서 실컷 수다도 떨고 맛있는 것도 먹고   \n",
       "39585  5fbb59c144697678c497b6d6                              많이 힘들거다   \n",
       "39586  5f0d6624b140144dfcff35c6  엊그제 갑자기 의식을 잃었어 나아지는가 싶더니 결국 죽고 말았지   \n",
       "39587  5fb88b9844697678c497adf1                           한 명이야 한 명!   \n",
       "\n",
       "              감정  감정레이블  Angry  Disgust  Fear  Happiness  Neutral  Sadness  \\\n",
       "0        Neutral      4      0        0     0          0        1        0   \n",
       "1        Neutral      4      0        0     0          0        1        0   \n",
       "2          Angry      0      1        0     0          0        0        0   \n",
       "3        Sadness      5      0        0     0          0        0        1   \n",
       "4        Sadness      5      0        0     0          0        0        1   \n",
       "...          ...    ...    ...      ...   ...        ...      ...      ...   \n",
       "39583    Neutral      4      0        0     0          0        1        0   \n",
       "39584  Happiness      3      0        0     0          1        0        0   \n",
       "39585    Disgust      1      0        1     0          0        0        0   \n",
       "39586    Sadness      5      0        0     0          0        0        1   \n",
       "39587    Neutral      4      0        0     0          0        1        0   \n",
       "\n",
       "       Surprise                          path  \n",
       "0             0  5e258fd1305bcf3ad153a6a4.wav  \n",
       "1             0  5e258fe2305bcf3ad153a6a5.wav  \n",
       "2             0  5e258ff5305bcf3ad153a6a6.wav  \n",
       "3             0  5e25902f305bcf3ad153a6a9.wav  \n",
       "4             0  5e27ec9f5807b852d9e01542.wav  \n",
       "...         ...                           ...  \n",
       "39583         0  5fb8e0834c55eb78bd7cdec2.wav  \n",
       "39584         0  5f8cd53af8fac448cc0a783a.wav  \n",
       "39585         0  5fbb59c144697678c497b6d6.wav  \n",
       "39586         0  5f0d6624b140144dfcff35c6.wav  \n",
       "39587         0  5fb88b9844697678c497adf1.wav  \n",
       "\n",
       "[39588 rows x 12 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "60a0f279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4402"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_csv)\n",
    "len(test_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe8cd48",
   "metadata": {},
   "source": [
    "```python\n",
    "train_csv.reset_index(inplace=True)\n",
    "test_csv.reset_index(inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f614c4",
   "metadata": {},
   "source": [
    "```python\n",
    "for i in range(len(train_csv)): #전체 파일의 개수만큼의 회수\n",
    "    if train_csv['감정레이블'][i] == 1 or train_csv['감정레이블'][i] == 2 or train_csv['감정레이블'][i] == 6: \n",
    "        train_csv.drop(index=i,inplace=True)\n",
    "        i-=1\n",
    "for i in range(len(test_csv)): #전체 파일의 개수만큼의 회수\n",
    "    if test_csv['감정레이블'][i] == 1 or test_csv['감정레이블'][i] == 2 or test_csv['감정레이블'][i] == 6: \n",
    "        test_csv.drop(index=i,inplace=True)\n",
    "        i-=1\n",
    "#무의미해진 원핫인코딩 컬럼도 제거        \n",
    "train_csv.drop(columns=['Disgust','Fear','Surprise'],inplace=True)\n",
    "test_csv.drop(columns=['Disgust','Fear','Surprise'],inplace=True)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de508de",
   "metadata": {},
   "source": [
    "```python\n",
    "train_csv.to_csv('d:/Data2/csv/train_csv_9.csv',index=False)\n",
    "test_csv.to_csv('d:/Data2/csv/test_csv_9.csv',index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43a6db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#감정 세 개를 제거한 새로운 csv를 만들고 폴더 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abb2930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 함수 설정\n",
    "class text_embedding():\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "  \n",
    "    def transform(self, X,raw_label):\n",
    "        embedding_model = SentenceTransformer(self.model_name)\n",
    "        embedding_vec = embedding_model.encode(raw_label)\n",
    "        X_val = np.concatenate((X, embedding_vec), axis = 1)\n",
    "        return X_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1238eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음성 모델\n",
    "def cnn_model(data):\n",
    "    model=Sequential()\n",
    "    model.add(InputLayer(shape=(data.shape[1],data.shape[2])))\n",
    "    model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "    model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "    model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=32, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(units=4, activation='softmax'))\n",
    "    model.compile(optimizer = 'adamW' , loss = 'categorical_crossentropy' , metrics = ['accuracy','f1_score']) #metrics 추가\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e46b7e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_f1(data,f1_score):\n",
    "    weighted_f1_score=0\n",
    "    class_data_count = np.sum(data,axis=0)\n",
    "    for i in range(len(f1_score)):\n",
    "        weighted_f1_score += (class_data_count[i]/sum(class_data_count))*f1_score[i]\n",
    "    return weighted_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4c6f7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptual_sharpness(audio_path, sr=16000, n_fft=400, hop_length=160):\n",
    "    # 음원 파일 로드\n",
    "    y= audio_path\n",
    "    sr = sr\n",
    "\n",
    "    # STFT 수행\n",
    "    D = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length))\n",
    "\n",
    "    # 주파수 대역별로 에너지 계산\n",
    "    energy = np.sum(D, axis=0)\n",
    "\n",
    "    # 고주파수 대역 성분 추출\n",
    "    high_freq_energy = energy[3000:8000]  # 예시로 3000Hz에서 6000Hz 사이의 주파수 대역을 고주파수 대역으로 설정\n",
    "\n",
    "    # Perceptual Sharpness 계산\n",
    "    sharpness = np.sum(np.log1p(high_freq_energy))\n",
    "\n",
    "    return sharpness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c742d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(data):\n",
    "    feature_list = []\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "        audio, sr = librosa.load(data[i], sr=16000)\n",
    "         ##Mel-spectrogram 구현\n",
    "        spectrogram = librosa.stft(audio, n_fft=400, hop_length= 160) \n",
    "        power_spectrogram = spectrogram**2\n",
    "        mel = librosa.feature.melspectrogram(S=power_spectrogram, sr=sr)\n",
    "        mel = librosa.power_to_db(np.abs(mel)**2)\n",
    "        #mfcc 구현\n",
    "        mfccs = librosa.feature.mfcc(S = mel, n_mfcc=100)\n",
    "        stft = np.abs(spectrogram)\n",
    "        chroma_stft = librosa.feature.chroma_stft(S=stft,hop_length=160)\n",
    "        rms = librosa.feature.rms(y=audio)\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
    "        spectral_bandwidths = librosa.feature.spectral_bandwidth(y=audio, sr=sr)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)\n",
    "        zero_crossing_rates = librosa.feature.zero_crossing_rate(y=audio)\n",
    "        chroma_cens = librosa.feature.chroma_cens(C=spectrogram, sr=sr)\n",
    "        tempo, _ = librosa.beat.beat_track(y=audio, sr=sr)\n",
    "        ps = perceptual_sharpness(audio)\n",
    "\n",
    "        try:\n",
    "            mfccs_mean = mfccs.mean(axis=1)\n",
    "            mfccs_var = mfccs.mean(axis=1)\n",
    "\n",
    "            for k in range(len(mfccs_mean)):\n",
    "                locals()[f'mfccs_mean_{k}'] = mfccs_mean[k]\n",
    "                locals()[f'mfccs_var_{k}'] = mfccs_var[k]\n",
    "                chroma_stft_mean = chroma_stft.mean()\n",
    "                chroma_stft_var = chroma_stft.var()\n",
    "                rms_mean = rms.mean()\n",
    "                rms_var = rms.var()\n",
    "                spectral_centroids_mean = spectral_centroids.mean()\n",
    "                spectral_centroids_var = spectral_centroids.var()\n",
    "                spectral_bandwidths_mean = spectral_bandwidths.mean()\n",
    "                spectral_bandwidths_var = spectral_bandwidths.var()\n",
    "                spectral_rolloff_mean = spectral_rolloff.mean()\n",
    "                spectral_rolloff_var = spectral_rolloff.var()\n",
    "                zero_crossing_rates_mean = zero_crossing_rates.mean()\n",
    "                zero_crossing_rates_var = zero_crossing_rates.var()\n",
    "                harmony_mean = chroma_cens.mean()\n",
    "                harmony_var = chroma_cens.var()\n",
    "                tempo_mean = tempo.mean()\n",
    "                tempo_var = tempo.var()\n",
    "                perceptual_sharpness_mean = ps.mean()\n",
    "                perceptual_sharpness_var = ps.var()\n",
    "        except Exception as e:\n",
    "            print(f'{i}번째 파일에서 문제 발생',e)\n",
    "\n",
    "            #합치기\n",
    "        features = np.array([])\n",
    "        for j in range(len(mfccs_mean)):\n",
    "            features = np.hstack((features,locals()[f'mfccs_mean_{j}'],locals()[f'mfccs_var_{j}']))\n",
    "\n",
    "        features = np.hstack((features,chroma_stft_mean,chroma_stft_var,rms_mean,rms_var,spectral_centroids_mean\n",
    "                             ,spectral_centroids_var, spectral_bandwidths_mean, spectral_bandwidths_var,spectral_rolloff_mean\n",
    "                             , spectral_rolloff_var,zero_crossing_rates_mean, zero_crossing_rates_var, harmony_mean, harmony_var\n",
    "                             , tempo_mean,tempo_var,perceptual_sharpness_mean,perceptual_sharpness_var))\n",
    "        augmenter = naa.NoiseAug() #데이터 증강을 위해 노이즈된 특성 추가\n",
    "        augmented_features = np.squeeze(augmenter.augment(features))\n",
    "        features =np.hstack((features,augmented_features))\n",
    "\n",
    "        feature_list.append(features)\n",
    "    feature_array  = np.array(feature_list).reshape(-1,len(feature_list[0]))\n",
    "    \n",
    "    return feature_array    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1d1fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature9 = extract_feature(glob.glob('d:/Data2/train/*.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "163fa499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('C:/Users/smhrd/Python/본인 생성 자료/파이썬/features/train_features_9.npy',feature9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb76ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_feature9 = extract_feature(glob.glob('d:/Data2/test/*.wav'))\n",
    "# np.save('C:/Users/smhrd/Python/본인 생성 자료/파이썬/features/test_features_9.npy',test_feature9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0fa669ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33219, 436)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature9.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ccb3fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본적인 사전 설정\n",
    "rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=5, min_lr=0.0000001) #learning rate 조절 \n",
    "modelpath = './model/model_{epoch:02d}-{val_accuracy:.4f}.keras'\n",
    "mcp = ModelCheckpoint(\n",
    "    modelpath,     #저장할 모델의 경로\n",
    "  monitor = 'val_accuracy', #val_acc를 기준으로 전보다 모델이 나아지는 걸 확인\n",
    " \n",
    "  save_best_only = True,    #나아진 결과만 저장\n",
    "#     save_weights_only=True , #이걸 써 줘야 weights.h5로 저장 가능하다.\n",
    "  verbose = 1               #과정을 출력\n",
    ")\n",
    "\n",
    "#전보다 나아지지 않으면 학습중단\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor = 'val_accuracy',\n",
    "    patience = 5      # 전보다 나아지지 않아도 실행할 횟수\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e7b7b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('./features/train_features_9.npy') \n",
    "X = np.real(X) #복소수 형태로 값이 나왔음\n",
    "y = train_csv.iloc[:,-5:-1]\n",
    "y= np.array(y)\n",
    "\n",
    "X_test = np.load('./features/test_features_9.npy')\n",
    "X_test = np.real(X_test)\n",
    "y_test = test_csv.iloc[:,-5:-1] #원핫인코딩된 데이터\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c564a7",
   "metadata": {},
   "source": [
    "```python\n",
    "pre_trained_models = 'jhgan/ko-sroberta-multitask'\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "\n",
    "# train_txt_embed = text_embedding(model_name = pre_trained_models)\n",
    "# X_train_embed = train_txt_embed.transform(X,train_csv['발화문'])\n",
    "X_train_embed = np.load('./txt_embed/X_train_embed--ko-sroberta-multitask_9.npy')  \n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_embed,y, random_state=0,  test_size = 0.1, stratify = y)\n",
    "\n",
    "sscaler_9 = scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "X_train_scaled = np.expand_dims(X_train_scaled,axis =1)\n",
    "X_valid_scaled = np.expand_dims(X_valid_scaled,axis =1)\n",
    "\n",
    "\n",
    "model = cnn_model(X_train_scaled)\n",
    "history=model.fit(\n",
    "    X_train_scaled, \n",
    "    y_train,batch_size=40, \n",
    "    epochs=50, \n",
    "    validation_data=(X_valid_scaled, y_valid),\n",
    "    callbacks=[rlrp,es,mcp]\n",
    ")\n",
    "valid_loss, valid_acc, valid_f1_score = model.evaluate(X_valid_scaled, y_valid, verbose=0)\n",
    "print(\"Pre-trained Model: \", pre_trained_models)\n",
    "print(\"Valid Accuracy: \",f'{valid_acc:.2f}')\n",
    "print(\"Valid Loss: \", f'{valid_loss:.2f}')\n",
    "print(\"Weighted F1Score: \", f'{weighted_f1(y_valid,valid_f1_score).numpy():.2f}')\n",
    "#     except Exception as e:    # 모든 예외의 에러 메시지를 출력할 때는 Exception을 사용\n",
    "#         print('예외가 발생했습니다.', e)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ebd5debb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33219, 1204)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c64b184c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./scaler/sscaler_9.pkl']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.save('./txt_embed/X_train_embed--ko-sroberta-multitask_9.npy',X_train_embed)\n",
    "joblib.dump(sscaler_9,'./scaler/sscaler_9.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "4f396b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('./model/model_01-0.7664.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "82a64e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained Model:  jhgan/ko-sroberta-multitask\n",
      "Test Accuracy:  0.67,   Test Loss:  0.79,   Weighted F1Score:  0.68   "
     ]
    }
   ],
   "source": [
    "# test_txt_embed = text_embedding(model_name = pre_trained_models)\n",
    "# X_test_embed = test_txt_embed.transform(X_test,test_csv['발화문'])\n",
    "X_test_embed = np.load('./txt_embed/X_test_embed--ko-sroberta-multitask_9.npy')  \n",
    "\n",
    "X_test_scaled = scaler.transform(X_test_embed)\n",
    "X_test_scaled = np.expand_dims(X_test_scaled,axis =1)\n",
    "\n",
    "test_loss, test_acc, test_f1_score = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(\"Pre-trained Model: \", pre_trained_models)\n",
    "print(\"Test Accuracy: \",f'{test_acc:.2f}',end=',   ')\n",
    "print(\"Test Loss: \", f'{test_loss:.2f}',end=',   ')\n",
    "print(\"Weighted F1Score: \", f'{weighted_f1(y_test,test_f1_score).numpy():.2f}',end='   ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "7dab1ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./txt_embed/X_test_embed--ko-sroberta-multitask_9.npy',X_test_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaa2e1e",
   "metadata": {},
   "source": [
    "### 모델별 데이터 기록\n",
    "- model_05-0.8016.keras  - 현재까지 best\n",
    "    - Test Accuracy:  0.73,   Test Loss:  0.82,   Weighted F1Score:  0.73  \n",
    "- \n",
    "- model_04-0.7998.keras\n",
    "    - Test Accuracy:  0.70,   Test Loss:  0.78,   Weighted F1Score:  0.71 \n",
    "- \n",
    "- model_02-0.7809.keras\n",
    "    - Test Accuracy:  0.71,   Test Loss:  0.76,   Weighted F1Score:  0.71\n",
    "- \n",
    "- model_01-0.7787.keras\n",
    "    - Test Accuracy:  0.70,   Test Loss:  0.78,   Weighted F1Score:  0.70   \n",
    "- \n",
    "- model_04-0.7932.keras\n",
    "    - Test Accuracy:  0.70,   Test Loss:  0.82,   Weighted F1Score:  0.71   \n",
    "- \n",
    "- model_03-0.7878.keras\n",
    "    - Test Accuracy:  0.70,   Test Loss:  0.78,   Weighted F1Score:  0.71\n",
    "- \n",
    "- model_02-0.7833.keras\n",
    "    - Test Accuracy:  0.70,   Test Loss:  0.76,   Weighted F1Score:  0.71 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d7ebf0",
   "metadata": {},
   "source": [
    "- 일단 73% 정도 성능이 나오니까 이 모델로 다시 구축\n",
    "- 향후 모델은 음성 특성 변경, 데이터 가중치 주기, 스케일러 변경 등으로 성능 올려보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e45c930",
   "metadata": {},
   "source": [
    "#### 1. 데이터 가중치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6285d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                     classes=np.unique(train_csv['감정레이블']), #라벨인코딩 된 클래스들\n",
    "                                     y=train_csv['감정레이블']) #라벨인코딩 데이터\n",
    "class_weights = dict(enumerate(class_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c80972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('./model/model_05-0.8016.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "e6850bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.6494 - f1_score: 0.6329 - loss: 0.8207\n",
      "Epoch 1: val_accuracy improved from -inf to 0.75075, saving model to ./model/model_01-0.7508.keras\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 35ms/step - accuracy: 0.6495 - f1_score: 0.6329 - loss: 0.8206 - val_accuracy: 0.7508 - val_f1_score: 0.7333 - val_loss: 0.6063 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m747/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7700 - f1_score: 0.7548 - loss: 0.5693\n",
      "Epoch 2: val_accuracy improved from 0.75075 to 0.76039, saving model to ./model/model_02-0.7604.keras\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.7700 - f1_score: 0.7548 - loss: 0.5693 - val_accuracy: 0.7604 - val_f1_score: 0.7368 - val_loss: 0.6070 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.7876 - f1_score: 0.7745 - loss: 0.5199\n",
      "Epoch 3: val_accuracy improved from 0.76039 to 0.76400, saving model to ./model/model_03-0.7640.keras\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.7876 - f1_score: 0.7745 - loss: 0.5199 - val_accuracy: 0.7640 - val_f1_score: 0.7465 - val_loss: 0.5695 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7993 - f1_score: 0.7882 - loss: 0.4817\n",
      "Epoch 4: val_accuracy improved from 0.76400 to 0.76942, saving model to ./model/model_04-0.7694.keras\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.7993 - f1_score: 0.7882 - loss: 0.4817 - val_accuracy: 0.7694 - val_f1_score: 0.7556 - val_loss: 0.5727 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m747/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8077 - f1_score: 0.7969 - loss: 0.4566\n",
      "Epoch 5: val_accuracy did not improve from 0.76942\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8076 - f1_score: 0.7969 - loss: 0.4566 - val_accuracy: 0.7670 - val_f1_score: 0.7556 - val_loss: 0.5579 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m747/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8136 - f1_score: 0.8056 - loss: 0.4236\n",
      "Epoch 6: val_accuracy did not improve from 0.76942\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8136 - f1_score: 0.8056 - loss: 0.4236 - val_accuracy: 0.7667 - val_f1_score: 0.7513 - val_loss: 0.6223 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8242 - f1_score: 0.8154 - loss: 0.4072\n",
      "Epoch 7: val_accuracy did not improve from 0.76942\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.8242 - f1_score: 0.8154 - loss: 0.4072 - val_accuracy: 0.7532 - val_f1_score: 0.7441 - val_loss: 0.6291 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m747/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8295 - f1_score: 0.8234 - loss: 0.3686\n",
      "Epoch 8: val_accuracy improved from 0.76942 to 0.77213, saving model to ./model/model_08-0.7721.keras\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.8295 - f1_score: 0.8234 - loss: 0.3687 - val_accuracy: 0.7721 - val_f1_score: 0.7505 - val_loss: 0.6359 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8400 - f1_score: 0.8301 - loss: 0.3544\n",
      "Epoch 9: val_accuracy improved from 0.77213 to 0.78326, saving model to ./model/model_09-0.7833.keras\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8400 - f1_score: 0.8301 - loss: 0.3544 - val_accuracy: 0.7833 - val_f1_score: 0.7733 - val_loss: 0.5741 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m747/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8575 - f1_score: 0.8526 - loss: 0.3286\n",
      "Epoch 10: val_accuracy did not improve from 0.78326\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8575 - f1_score: 0.8526 - loss: 0.3286 - val_accuracy: 0.7748 - val_f1_score: 0.7581 - val_loss: 0.6554 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8572 - f1_score: 0.8531 - loss: 0.3036\n",
      "Epoch 11: val_accuracy did not improve from 0.78326\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8572 - f1_score: 0.8531 - loss: 0.3036 - val_accuracy: 0.7793 - val_f1_score: 0.7617 - val_loss: 0.6766 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8685 - f1_score: 0.8631 - loss: 0.2947\n",
      "Epoch 12: val_accuracy improved from 0.78326 to 0.78657, saving model to ./model/model_12-0.7866.keras\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.8685 - f1_score: 0.8631 - loss: 0.2947 - val_accuracy: 0.7866 - val_f1_score: 0.7735 - val_loss: 0.6441 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m747/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8724 - f1_score: 0.8682 - loss: 0.2772\n",
      "Epoch 13: val_accuracy did not improve from 0.78657\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.8724 - f1_score: 0.8682 - loss: 0.2773 - val_accuracy: 0.7815 - val_f1_score: 0.7694 - val_loss: 0.7294 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8818 - f1_score: 0.8768 - loss: 0.2605\n",
      "Epoch 14: val_accuracy did not improve from 0.78657\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.8818 - f1_score: 0.8768 - loss: 0.2605 - val_accuracy: 0.7775 - val_f1_score: 0.7685 - val_loss: 0.7769 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m747/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8852 - f1_score: 0.8823 - loss: 0.2471\n",
      "Epoch 15: val_accuracy did not improve from 0.78657\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.8852 - f1_score: 0.8823 - loss: 0.2471 - val_accuracy: 0.7830 - val_f1_score: 0.7705 - val_loss: 0.7535 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8933 - f1_score: 0.8898 - loss: 0.2282\n",
      "Epoch 16: val_accuracy did not improve from 0.78657\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.8933 - f1_score: 0.8898 - loss: 0.2283 - val_accuracy: 0.7745 - val_f1_score: 0.7538 - val_loss: 0.8207 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9020 - f1_score: 0.9002 - loss: 0.2171\n",
      "Epoch 17: val_accuracy did not improve from 0.78657\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9020 - f1_score: 0.9001 - loss: 0.2171 - val_accuracy: 0.7691 - val_f1_score: 0.7519 - val_loss: 0.8002 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9052 - f1_score: 0.9035 - loss: 0.2137\n",
      "Epoch 18: val_accuracy improved from 0.78657 to 0.79169, saving model to ./model/model_18-0.7917.keras\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.9052 - f1_score: 0.9035 - loss: 0.2137 - val_accuracy: 0.7917 - val_f1_score: 0.7753 - val_loss: 0.7935 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m747/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9171 - f1_score: 0.9165 - loss: 0.1798\n",
      "Epoch 19: val_accuracy did not improve from 0.79169\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.9171 - f1_score: 0.9165 - loss: 0.1799 - val_accuracy: 0.7830 - val_f1_score: 0.7651 - val_loss: 0.8866 - learning_rate: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50\n",
      "\u001b[1m747/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9145 - f1_score: 0.9112 - loss: 0.1880\n",
      "Epoch 20: val_accuracy did not improve from 0.79169\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9144 - f1_score: 0.9112 - loss: 0.1881 - val_accuracy: 0.7824 - val_f1_score: 0.7619 - val_loss: 0.9484 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m747/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9228 - f1_score: 0.9198 - loss: 0.1717\n",
      "Epoch 21: val_accuracy did not improve from 0.79169\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9228 - f1_score: 0.9198 - loss: 0.1718 - val_accuracy: 0.7775 - val_f1_score: 0.7633 - val_loss: 0.8774 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9251 - f1_score: 0.9213 - loss: 0.1611\n",
      "Epoch 22: val_accuracy did not improve from 0.79169\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.9251 - f1_score: 0.9213 - loss: 0.1611 - val_accuracy: 0.7697 - val_f1_score: 0.7539 - val_loss: 0.9810 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9283 - f1_score: 0.9262 - loss: 0.1586\n",
      "Epoch 23: val_accuracy did not improve from 0.79169\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9283 - f1_score: 0.9262 - loss: 0.1586 - val_accuracy: 0.7778 - val_f1_score: 0.7590 - val_loss: 1.0848 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9284 - f1_score: 0.9259 - loss: 0.1581\n",
      "Epoch 24: val_accuracy did not improve from 0.79169\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.9284 - f1_score: 0.9259 - loss: 0.1581 - val_accuracy: 0.7787 - val_f1_score: 0.7595 - val_loss: 0.9903 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m747/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9297 - f1_score: 0.9255 - loss: 0.1644\n",
      "Epoch 25: val_accuracy did not improve from 0.79169\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.9297 - f1_score: 0.9255 - loss: 0.1644 - val_accuracy: 0.7857 - val_f1_score: 0.7659 - val_loss: 1.0818 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m747/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9422 - f1_score: 0.9404 - loss: 0.1308\n",
      "Epoch 26: val_accuracy did not improve from 0.79169\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9422 - f1_score: 0.9404 - loss: 0.1309 - val_accuracy: 0.7634 - val_f1_score: 0.7541 - val_loss: 1.0834 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m747/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9397 - f1_score: 0.9366 - loss: 0.1380\n",
      "Epoch 27: val_accuracy did not improve from 0.79169\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.9397 - f1_score: 0.9366 - loss: 0.1381 - val_accuracy: 0.7809 - val_f1_score: 0.7653 - val_loss: 1.1652 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m747/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9415 - f1_score: 0.9387 - loss: 0.1345\n",
      "Epoch 28: val_accuracy did not improve from 0.79169\n",
      "\u001b[1m748/748\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.9415 - f1_score: 0.9387 - loss: 0.1345 - val_accuracy: 0.7736 - val_f1_score: 0.7603 - val_loss: 1.1762 - learning_rate: 0.0010\n",
      "Pre-trained Model:  jhgan/ko-sroberta-multitask\n",
      "Valid Accuracy:  0.77\n",
      "Valid Loss:  1.18\n",
      "Weighted F1Score:  0.78\n"
     ]
    }
   ],
   "source": [
    "# ```python\n",
    "pre_trained_models = 'jhgan/ko-sroberta-multitask'\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "\n",
    "# train_txt_embed = text_embedding(model_name = pre_trained_models)\n",
    "# X_train_embed = train_txt_embed.transform(X,train_csv['발화문'])\n",
    "X_train_embed = np.load('./txt_embed/X_train_embed--ko-sroberta-multitask_9.npy')  \n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_embed,y, random_state=0,  test_size = 0.1, stratify = y)\n",
    "\n",
    "sscaler_9 = scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "X_train_scaled = np.expand_dims(X_train_scaled,axis =1)\n",
    "X_valid_scaled = np.expand_dims(X_valid_scaled,axis =1)\n",
    "\n",
    "\n",
    "model = cnn_model(X_train_scaled)\n",
    "history=model.fit(\n",
    "    X_train_scaled, \n",
    "    y_train,batch_size=40, \n",
    "    epochs=50, \n",
    "    validation_data=(X_valid_scaled, y_valid),\n",
    "    class_weight=class_weights , #여기 추가!\n",
    "    callbacks=[rlrp,es,mcp]\n",
    ")\n",
    "valid_loss, valid_acc, valid_f1_score = model.evaluate(X_valid_scaled, y_valid, verbose=0)\n",
    "print(\"Pre-trained Model: \", pre_trained_models)\n",
    "print(\"Valid Accuracy: \",f'{valid_acc:.2f}')\n",
    "print(\"Valid Loss: \", f'{valid_loss:.2f}')\n",
    "print(\"Weighted F1Score: \", f'{weighted_f1(y_valid,valid_f1_score).numpy():.2f}')\n",
    "#     except Exception as e:    # 모든 예외의 에러 메시지를 출력할 때는 Exception을 사용\n",
    "#         print('예외가 발생했습니다.', e)\n",
    "\n",
    "# ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f51a2c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('./model/model_05-0.7950.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5399dc62",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_txt_embed = text_embedding(model_name = pre_trained_models)\n",
    "# X_test_embed = test_txt_embed.transform(X_test,test_csv['발화문'])\n",
    "X_test_embed = np.load('./txt_embed/X_test_embed--ko-sroberta-multitask_9.npy')  \n",
    "\n",
    "X_test_scaled = scaler.transform(X_test_embed)\n",
    "X_test_scaled = np.expand_dims(X_test_scaled,axis =1)\n",
    "\n",
    "test_loss, test_acc, test_f1_score = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(\"Pre-trained Model: \", pre_trained_models)\n",
    "print(\"Test Accuracy: \",f'{test_acc:.2f}',end=',   ')\n",
    "print(\"Test Loss: \", f'{test_loss:.2f}',end=',   ')\n",
    "print(\"Weighted F1Score: \", f'{weighted_f1(y_test,test_f1_score).numpy():.2f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac62bbf",
   "metadata": {},
   "source": [
    "#### 1. 가중치 주기 : 결과\n",
    "- 데이터에 가중치를 줬더니 오히려 test 성능이 떨어졌다...?\n",
    "- 테스트set도 클래스별 데이터 수가 훈련 set과 동일한 비율로 나뉘어져 있으므로 그 영향이 있을 수 있음.\n",
    "- 각 감정의 비율이 동일한 새로운 테스트 셋 구축하여 실험해볼 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "369ed4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407\n",
      "3\n",
      "1686\n",
      "5\n",
      "796\n",
      "0\n",
      "804\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(test_csv['감정레이블'].unique()): \n",
    "    print(len(test_csv[test_csv['감정레이블']==data]))\n",
    "    print(data)\n",
    "\n",
    "#Angry, Happiness, Neutral, Sadness 중 Happiness가 가장 작은 407개.\n",
    "#다른 감정도 407개만 남겨두고 싹 날려버리자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c88e21",
   "metadata": {},
   "source": [
    "```python\n",
    "angry_cnt = neutral_cnt = sad_cnt = 0\n",
    "list_to_move=[]\n",
    "for i in range(len(test_csv)):\n",
    "    if test_csv.iloc[i]['감정레이블']==3:\n",
    "        continue\n",
    "    elif test_csv.iloc[i]['감정레이블']==0:\n",
    "        if angry_cnt<407:\n",
    "            angry_cnt+=1\n",
    "        else:\n",
    "            list_to_move.append(test_csv.iloc[i]['path'])\n",
    "    elif test_csv.iloc[i]['감정레이블']==4:\n",
    "        if neutral_cnt<407:\n",
    "            neutral_cnt+=1\n",
    "        else:\n",
    "            list_to_move.append(test_csv.iloc[i]['path'])\n",
    "    elif test_csv.iloc[i]['감정레이블']==5:\n",
    "        if sad_cnt<407:\n",
    "            sad_cnt+=1\n",
    "        else:\n",
    "            list_to_move.append(test_csv.iloc[i]['path'])\n",
    "\n",
    "\n",
    "for i,data in enumerate(test_csv.iloc):\n",
    "    if data['path'] in list_to_move:\n",
    "        shutil.move(test_audio_path+data['path'],'d:/Data2/test_moreThanMinClass/'+data['path'])\n",
    "\n",
    "for i in range(len(test_csv['path'])):\n",
    "    if test_csv['path'][i] in list_to_move:\n",
    "        test_csv.drop(index=i,inplace=True)\n",
    "\n",
    "test_csv.reset_index(drop=True,inplace=True)\n",
    "test_csv.to_csv('d:/Data2/csv/test_ClassCountEqual.csv',index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4b330f",
   "metadata": {},
   "source": [
    "- 비교 대상 : \n",
    "    - 현재 테스트셋에 최적화된 model_05-0.8016.keras\n",
    "    - 가중치 부여 후 나온 model_01-0.7489.keras, model_02-0.7604.keras, model_03-0.7727.keras, model_05-0.7763.keras, model_07-0.7866.keras, model_11-0.7887.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9d75f6",
   "metadata": {},
   "source": [
    "```python\n",
    "X_test = extract_feature(glob.glob(test_audio_path+'*.wav')) # X_test 파일 수가 바뀌었으므로 다시 특징 추출\n",
    "np.save('./features/test_features_9_classCountEqual.npy',X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "949c1c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = pd.read_csv('d:/Data2/csv/test_ClassCountEqual.csv')\n",
    "X_test = np.load('./features/test_features_9_classCountEqual.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "a051c509",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_csv.iloc[:,-5:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "ed44238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('./model/model_11-0.7887.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "0de03431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1628, 436)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "41826bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained Model:  jhgan/ko-sroberta-multitask\n",
      "Test Accuracy:  0.69,   Test Loss:  1.20,   Weighted F1Score:  0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smhrd\\AppData\\Local\\Temp\\ipykernel_12704\\799221314.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  weighted_f1_score += (class_data_count[i]/sum(class_data_count))*f1_score[i]\n"
     ]
    }
   ],
   "source": [
    "# X_test = np.real(X_test)\n",
    "# test_txt_embed = text_embedding(model_name = pre_trained_models)\n",
    "# X_test_embed = test_txt_embed.transform(X_test,test_csv['발화문'])\n",
    "X_test_embed = np.load('./txt_embed/X_test_embed--ko-sroberta-multitask_9_classCountEqual.npy')  \n",
    "# np.save('./txt_embed/X_test_embed--ko-sroberta-multitask_9_classCountEqual.npy',X_test_embed) #\n",
    "\n",
    "scaler = joblib.load('./scaler/sscaler_9.pkl')\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test_embed)\n",
    "X_test_scaled = np.expand_dims(X_test_scaled,axis =1)\n",
    "\n",
    "test_loss, test_acc, test_f1_score = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(\"Pre-trained Model: \", pre_trained_models)\n",
    "print(\"Test Accuracy: \",f'{test_acc:.2f}',end=',   ')\n",
    "print(\"Test Loss: \", f'{test_loss:.2f}',end=',   ')\n",
    "print(\"Weighted F1Score: \", f'{weighted_f1(y_test,test_f1_score).numpy():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d158d33",
   "metadata": {},
   "source": [
    "- model_05-0.8016.keras : Test Accuracy:  0.69,   Test Loss:  0.91,   Weighted F1Score:  0.70\n",
    "- model_01-0.7489.keras : Test Accuracy:  0.67,   Test Loss:  0.85,   Weighted F1Score:  0.67\n",
    "- model_02-0.7604.keras : Test Accuracy:  0.68,   Test Loss:  0.82,   Weighted F1Score:  0.67\n",
    "- model_03-0.7727.keras : Test Accuracy:  0.68,   Test Loss:  0.81,   Weighted F1Score:  0.68\n",
    "- model_05-0.7763.keras : Test Accuracy:  0.68,   Test Loss:  0.81,   Weighted F1Score:  0.68\n",
    "- model_07-0.7866.keras : Test Accuracy:  0.68,   Test Loss:  0.86,   Weighted F1Score:  0.68\n",
    "- model_11-0.7887.keras : Test Accuracy:  0.69,   Test Loss:  1.20,   Weighted F1Score:  0.70\n",
    "\n",
    "#### 클래스별 데이터 수를 일치시켜도 가중치를 준 모델이 안 준 모델보다 성능이 딱히 좋지 않다...? EarlyStopping의 영향일 가능성을 감안하여 마지막으로 EarlyStopping 수를 10으로 늘려보기로  함. 이번에도 별로 안 좋으면 포기."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0edf1f6",
   "metadata": {},
   "source": [
    "### EarlyStopping Patience를 10으로 조정한 결과 :\n",
    "- model_01-0.7508.keras, model_02-0.7604.keras, model_03-0.7640.keras, model_04-0.7694.keras, model_08-0.7721.keras, model_09-0.7833.keras, model_12-0.7866.keras, model_18-0.7917.keras 이렇게 8개 모델이 나옴.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "63934b48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  model_05-0.8016.keras\n",
      "Test Accuracy:  0.69,   Test Loss:  0.91,   Weighted F1Score:  0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smhrd\\AppData\\Local\\Temp\\ipykernel_12704\\799221314.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  weighted_f1_score += (class_data_count[i]/sum(class_data_count))*f1_score[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  model_01-0.7508.keras\n",
      "Test Accuracy:  0.68,   Test Loss:  0.83,   Weighted F1Score:  0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smhrd\\AppData\\Local\\Temp\\ipykernel_12704\\799221314.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  weighted_f1_score += (class_data_count[i]/sum(class_data_count))*f1_score[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  model_02-0.7604.keras\n",
      "Test Accuracy:  0.68,   Test Loss:  0.79,   Weighted F1Score:  0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smhrd\\AppData\\Local\\Temp\\ipykernel_12704\\799221314.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  weighted_f1_score += (class_data_count[i]/sum(class_data_count))*f1_score[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  model_03-0.7640.keras\n",
      "Test Accuracy:  0.69,   Test Loss:  0.81,   Weighted F1Score:  0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smhrd\\AppData\\Local\\Temp\\ipykernel_12704\\799221314.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  weighted_f1_score += (class_data_count[i]/sum(class_data_count))*f1_score[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  model_04-0.7694.keras\n",
      "Test Accuracy:  0.68,   Test Loss:  0.88,   Weighted F1Score:  0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smhrd\\AppData\\Local\\Temp\\ipykernel_12704\\799221314.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  weighted_f1_score += (class_data_count[i]/sum(class_data_count))*f1_score[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  model_08-0.7721.keras\n",
      "Test Accuracy:  0.70,   Test Loss:  0.94,   Weighted F1Score:  0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smhrd\\AppData\\Local\\Temp\\ipykernel_12704\\799221314.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  weighted_f1_score += (class_data_count[i]/sum(class_data_count))*f1_score[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  model_09-0.7833.keras\n",
      "Test Accuracy:  0.69,   Test Loss:  0.90,   Weighted F1Score:  0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smhrd\\AppData\\Local\\Temp\\ipykernel_12704\\799221314.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  weighted_f1_score += (class_data_count[i]/sum(class_data_count))*f1_score[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  model_12-0.7866.keras\n",
      "Test Accuracy:  0.70,   Test Loss:  1.05,   Weighted F1Score:  0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smhrd\\AppData\\Local\\Temp\\ipykernel_12704\\799221314.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  weighted_f1_score += (class_data_count[i]/sum(class_data_count))*f1_score[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  model_18-0.7917.keras\n",
      "Test Accuracy:  0.69,   Test Loss:  1.53,   Weighted F1Score:  0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smhrd\\AppData\\Local\\Temp\\ipykernel_12704\\799221314.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  weighted_f1_score += (class_data_count[i]/sum(class_data_count))*f1_score[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  model_03-0.7899.keras\n",
      "Test Accuracy:  0.70,   Test Loss:  0.77,   Weighted F1Score:  0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smhrd\\AppData\\Local\\Temp\\ipykernel_12704\\799221314.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  weighted_f1_score += (class_data_count[i]/sum(class_data_count))*f1_score[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  model_12-0.7914.keras\n",
      "Test Accuracy:  0.69,   Test Loss:  1.18,   Weighted F1Score:  0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smhrd\\AppData\\Local\\Temp\\ipykernel_12704\\799221314.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  weighted_f1_score += (class_data_count[i]/sum(class_data_count))*f1_score[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  model_14-0.7965.keras\n",
      "Test Accuracy:  0.70,   Test Loss:  1.29,   Weighted F1Score:  0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smhrd\\AppData\\Local\\Temp\\ipykernel_12704\\799221314.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  weighted_f1_score += (class_data_count[i]/sum(class_data_count))*f1_score[i]\n"
     ]
    }
   ],
   "source": [
    "# X_test = np.real(X_test)\n",
    "# test_txt_embed = text_embedding(model_name = pre_trained_models)\n",
    "# X_test_embed = test_txt_embed.transform(X_test,test_csv['발화문'])\n",
    "X_test_embed = np.load('./txt_embed/X_test_embed--ko-sroberta-multitask_9_classCountEqual.npy')  \n",
    "# np.save('./txt_embed/X_test_embed--ko-sroberta-multitask_9_classCountEqual.npy',X_test_embed) #\n",
    "\n",
    "scaler = joblib.load('./scaler/sscaler_9.pkl')\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test_embed)\n",
    "X_test_scaled = np.expand_dims(X_test_scaled,axis =1)\n",
    "\n",
    "model_list = ['model_05-0.8016.keras','model_01-0.7508.keras', 'model_02-0.7604.keras', 'model_03-0.7640.keras', 'model_04-0.7694.keras',\n",
    "                'model_08-0.7721.keras', 'model_09-0.7833.keras', 'model_12-0.7866.keras', 'model_18-0.7917.keras','model_03-0.7899.keras',\n",
    "              'model_12-0.7914.keras','model_14-0.7965.keras']\n",
    "for mod in model_list:\n",
    "    model = keras.models.load_model(f'./model/{mod}')\n",
    "\n",
    "    test_loss, test_acc, test_f1_score = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "    print(\"Model: \", mod)\n",
    "    print(\"Test Accuracy: \",f'{test_acc:.2f}',end=',   ')\n",
    "    print(\"Test Loss: \", f'{test_loss:.2f}',end=',   ')\n",
    "    print(\"Weighted F1Score: \", f'{weighted_f1(y_test,test_f1_score).numpy():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6e6b3f",
   "metadata": {},
   "source": [
    "#### 결과 : \n",
    "- 가중치를 주었음에도 불구하고 model_05-0.8016.keras보다 그다지 뚜렷한 성능적 우위를 보이지 못함\n",
    "- 그나마 model_02-0.7604.keras, model_03-0.7640.keras은 비슷한 정확도에 현저하게 낮은 loss를 보이므로 가중치를 주는 것이 의미가 없지는 않을 듯."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe246063",
   "metadata": {},
   "source": [
    "#### 2. 스케일러 변경\n",
    "- 기존 데이터는 StandardScaler로 작업됨\n",
    "- 스케일러가 바뀌면 성능도 바뀔 것인지..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d5358384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.preprocessing._data.StandardScaler'>\n",
      "<class 'sklearn.preprocessing._data.MinMaxScaler'>\n",
      "<class 'sklearn.preprocessing._data.RobustScaler'>\n"
     ]
    }
   ],
   "source": [
    "scaler_list = [StandardScaler(),MinMaxScaler(), RobustScaler()]\n",
    "for i in scaler_list:\n",
    "    scaler = i\n",
    "    print(type(i))\n",
    "#이런 식으로 작동이 된다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44481d4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m X_train_scaled \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(X_train_scaled,axis \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     20\u001b[0m X_valid_scaled \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(X_valid_scaled,axis \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m model \u001b[38;5;241m=\u001b[39m cnn_model(X_train_scaled)\n\u001b[0;32m     24\u001b[0m history\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     25\u001b[0m     X_train_scaled, \n\u001b[0;32m     26\u001b[0m     y_train,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[rlrp,mcp] \u001b[38;5;66;03m#공평한 비교를 위해 earlyStopping 삭제\u001b[39;00m\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m valid_loss, valid_acc, valid_f1_score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_valid_scaled, y_valid, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cnn_model' is not defined"
     ]
    }
   ],
   "source": [
    "pre_trained_models = 'jhgan/ko-sroberta-multitask'\n",
    "scaler_list = [StandardScaler(),MinMaxScaler(), RobustScaler()] #StandardScaler는 이미 결과치가 나왔으므로 나머지 두 스케일러로\n",
    "scaler_name_list = ['s','m','r']\n",
    "\n",
    "\n",
    "\n",
    "# train_txt_embed = text_embedding(model_name = pre_trained_models)\n",
    "# X_train_embed = train_txt_embed.transform(X,train_csv['발화문'])\n",
    "X_train_embed = np.load('./txt_embed/X_train_embed--ko-sroberta-multitask_9.npy')  \n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_embed,y, random_state=0,  test_size = 0.1, stratify = y) \n",
    "\n",
    "for i in range(len(scaler_list)):\n",
    "    scaler = scaler_list[i]\n",
    "    locals()[f'{scaler_name_list[i]}scaler_9'] = scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "    X_train_scaled = np.expand_dims(X_train_scaled,axis =1)\n",
    "    X_valid_scaled = np.expand_dims(X_valid_scaled,axis =1)\n",
    "\n",
    "\n",
    "    model = cnn_model(X_train_scaled)\n",
    "    history=model.fit(\n",
    "        X_train_scaled, \n",
    "        y_train,batch_size=40, \n",
    "        epochs=20, #EarlyStopping 삭제했으므로 epochs는 20으로 줄임\n",
    "        validation_data=(X_valid_scaled, y_valid),\n",
    "        class_weight=class_weights , #여기 추가!\n",
    "        callbacks=[rlrp,mcp] #공평한 비교를 위해 earlyStopping 삭제\n",
    "    )\n",
    "    valid_loss, valid_acc, valid_f1_score = model.evaluate(X_valid_scaled, y_valid, verbose=0)\n",
    "    print(\"Pre-trained Model: \", pre_trained_models)\n",
    "    print(\"Valid Accuracy: \",f'{valid_acc:.2f}')\n",
    "    print(\"Valid Loss: \", f'{valid_loss:.2f}')\n",
    "    print(\"Weighted F1Score: \", f'{weighted_f1(y_valid,valid_f1_score).numpy():.2f}')\n",
    "\n",
    "\n",
    "    X_test_embed = np.load('./txt_embed/X_test_embed--ko-sroberta-multitask_9_classCountEqual.npy')\n",
    "    #test 데이터는 클래스별 값을 동일하게 한 상태\n",
    "\n",
    "    X_test_scaled = scaler.transform(X_test_embed)\n",
    "    X_test_scaled = np.expand_dims(X_test_scaled,axis =1)\n",
    "\n",
    "    test_loss, test_acc, test_f1_score = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "    print(\"Pre-trained Model: \", pre_trained_models)\n",
    "    print(\"Test Accuracy: \",f'{test_acc:.2f}',end=',   ')\n",
    "    print(\"Test Loss: \", f'{test_loss:.2f}',end=',   ')\n",
    "    print(\"Weighted F1Score: \", f'{weighted_f1(y_test,test_f1_score).numpy():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c7ebae",
   "metadata": {},
   "source": [
    "####  결과:\n",
    "- StandardScaler 외의 다른 스케일러는 이 데이터에 적합하지 않은 것으로 보임. StandardScaler 사용하자\n",
    "- 번외로 여기서 나온 모델들 돌려본 결과  model_03-0.7899.keras이 지금까지 모델들 중에 가장 우수한 성능을 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "af075487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29897, 1204)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "e2ab6652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting eli5\n",
      "  Downloading eli5-0.13.0.tar.gz (216 kB)\n",
      "     ---------------------------------------- 0.0/216.2 kB ? eta -:--:--\n",
      "     - -------------------------------------- 10.2/216.2 kB ? eta -:--:--\n",
      "     -------------------------------------  215.0/216.2 kB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 216.2/216.2 kB 2.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: attrs>17.1.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from eli5) (23.1.0)\n",
      "Requirement already satisfied: jinja2>=3.0.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from eli5) (3.1.3)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from eli5) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from eli5) (1.12.0)\n",
      "Requirement already satisfied: six in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from eli5) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from eli5) (1.4.0)\n",
      "Requirement already satisfied: graphviz in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from eli5) (0.20.1)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from eli5) (0.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from jinja2>=3.0.0->eli5) (2.1.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20->eli5) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20->eli5) (2.2.0)\n",
      "Building wheels for collected packages: eli5\n",
      "  Building wheel for eli5 (setup.py): started\n",
      "  Building wheel for eli5 (setup.py): finished with status 'done'\n",
      "  Created wheel for eli5: filename=eli5-0.13.0-py2.py3-none-any.whl size=107792 sha256=a36b93a4ae6fa8925d2b7bb4a540430ac1ef8da82cc253bb8f9e317c28cc14ee\n",
      "  Stored in directory: c:\\users\\smhrd\\appdata\\local\\pip\\cache\\wheels\\ec\\68\\a9\\de7d374ecb6f53462ce0eec8326fbab91b6228c82e67428d0d\n",
      "Successfully built eli5\n",
      "Installing collected packages: eli5\n",
      "Successfully installed eli5-0.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c318e5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 63ms/step - accuracy: 0.5720 - loss: 0.9764 - val_accuracy: 0.7757 - val_loss: 0.5765\n"
     ]
    }
   ],
   "source": [
    "X_train_embed = np.load('./txt_embed/X_train_embed--ko-sroberta-multitask_9.npy')  \n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_embed,y, random_state=0,  test_size = 0.1, stratify = y)\n",
    "\n",
    "sscaler_9 = scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "X_train_scaled = np.expand_dims(X_train_scaled,axis =1)\n",
    "X_valid_scaled = np.expand_dims(X_valid_scaled,axis =1)\n",
    "\n",
    "\n",
    "model = cnn_model(X_train_scaled)\n",
    "history=model.fit(\n",
    "    X_train_scaled, \n",
    "    y_train,batch_size=4000, \n",
    "    epochs=1, \n",
    "    validation_data=(X_valid_scaled, y_valid),\n",
    "#     class_weight=class_weights , #여기 추가!\n",
    "#     callbacks=[rlrp,es,mcp]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbacc77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "f6dfd2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3322, 1204)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf32f1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\smhrd\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smhrd\\anaconda3\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:99: UserWarning: Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n",
      "  warnings.warn(\"Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'as_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m X_train_sample \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(X_train_sample,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# DeepExplainer 사용\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mDeepExplainer(model, X_train_sample)\n\u001b[0;32m      6\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mshap_values(X_valid)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# SHAP 값을 시각화\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\shap\\explainers\\_deep\\__init__.py:90\u001b[0m, in \u001b[0;36mDeepExplainer.__init__\u001b[1;34m(self, model, data, session, learning_phase_flags)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(model, masker)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainer \u001b[38;5;241m=\u001b[39m TFDeep(model, data, session, learning_phase_flags)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m framework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpytorch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainer \u001b[38;5;241m=\u001b[39m PyTorchDeep(model, data)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:172\u001b[0m, in \u001b[0;36mTFDeep.__init__\u001b[1;34m(self, model, data, session, learning_phase_flags)\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphi_symbolics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m     noutputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_output\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mas_list()[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m noutputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphi_symbolics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(noutputs)]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'as_list'"
     ]
    }
   ],
   "source": [
    "X_train_sample = X_train[:100]  # 일부 샘플만 사용하여 SHAP 계산 (속도 문제)\n",
    "X_train_sample = np.expand_dims(X_train_sample,axis=1)\n",
    "\n",
    "# DeepExplainer 사용\n",
    "explainer = shap.DeepExplainer(model, X_train_sample)\n",
    "shap_values = explainer.shap_values(X_valid)\n",
    "\n",
    "# SHAP 값을 시각화\n",
    "shap.summary_plot(shap_values, X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "3671db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_shape = model.output_shape[0] if isinstance(model.output_shape, tuple) else model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "5585d6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tf-keras) (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.28.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (60.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.38.4)\n",
      "Requirement already satisfied: rich in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (13.4.2)\n",
      "Requirement already satisfied: namex in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.0.7)\n",
      "Requirement already satisfied: optree in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.1.0)\n",
      "Downloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.1/1.7 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.5/1.7 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.0/1.7 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.7/1.7 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 7.8 MB/s eta 0:00:00\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "6503d06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e720b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
