{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31b4aefb",
   "metadata": {},
   "source": [
    "## 목표\n",
    "\n",
    "- Kobert-Transformers 대신 KoELECTRA를 사용해 CNN 모델과 각각 특징을 추출하여 소프트보팅 앙상블 모델을 만든다\n",
    "- KFold를 이용해 다양한 학습을 하게 하여 성능을 높인다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebce3ce",
   "metadata": {},
   "source": [
    "### 순서\n",
    "- 1. 필요 모듈들 설치 및 임포트\n",
    "- 2. 기본 데이터 불러오기\n",
    "- 3. 필요한 함수들 설정(모델 포함)\n",
    "- 4. 모델 매개변수 설정\n",
    "- 5. 모델학습\n",
    "- 6. 평가\n",
    "- 7. 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549121be",
   "metadata": {},
   "source": [
    "#### 1. 필요 모듈들 설치 및 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74c134dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nlpaug in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (1.1.11)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from nlpaug) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from nlpaug) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from nlpaug) (2.31.0)\n",
      "Requirement already satisfied: gdown>=4.0.0 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from nlpaug) (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (3.13.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.66.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tqdm->gdown>=4.0.0->nlpaug) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# !pip install scikeras --users\n",
    "# !pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51a14531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "# import librosa.display\n",
    "# import IPython.display as ipd\n",
    "# from IPython.display import Audio\n",
    "\n",
    "# from random import randint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import pydub #오디오 파일을 다루는 라이브러리 : 변환 조작 재생 분석 등\n",
    "# import gTTS # 텍스트를 음성으로 변환하는 라이브러리. 나는 당장 쓸 일 없을 듯?\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "# import np_utils\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# import nlpaug.augmenter.audio as naa\n",
    "# import wave\n",
    "\n",
    "import glob\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "\n",
    "import winsound as ws\n",
    "import json\n",
    "# import shutil\n",
    "# import sys\n",
    "# import logging\n",
    "# import unicodedata\n",
    "# from shutil import copyfile\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"always\")\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler, MinMaxScaler, RobustScaler, FunctionTransformer\n",
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import recall_score,precision_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from scikeras.wrappers import KerasClassifier \n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "# from sklearn.svm import SVC \n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import joblib\n",
    "from joblib import dump, load\n",
    "# from xgboost import XGBClassifier \n",
    "\n",
    "# import keras\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint,EarlyStopping, Callback\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense, Conv1D, MaxPooling1D, Flatten, Dropout, concatenate\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.utils import to_categorical\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from keras.metrics import Precision, Recall, F1Score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# np.bool = np.bool_\n",
    "\n",
    "# import mxnet\n",
    "# import gluonnlp as nlp\n",
    "\n",
    "# from transformers import AdamW\n",
    "# from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "# from transformers import BertModel\n",
    "\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift # 음성 데이터 증강용\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc94c37",
   "metadata": {},
   "source": [
    "#### 2. 기본 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f382e7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('e:/Data2/csv/train.csv')\n",
    "test_csv = pd.read_csv('e:/Data2/csv/test.csv')\n",
    "exam_list = ['5fbe364744697678c497c07f','5fbe361844697678c497c07e']\n",
    "\n",
    "train_audio_path = 'e:/Data2/train/'\n",
    "test_audio_path = 'e:/Data2/test/'\n",
    "exam_audio_path = 'e:/Data2/exam/'\n",
    "\n",
    "# X = np.load('./features/train_features_6.npy')\n",
    "X = np.load('./features/train_features_8.npy')  #6번 특징이 별로 안 좋을 경우 시도\n",
    "X = np.real(X) #복소수 형태로 값이 나왔음\n",
    "y = train_csv.iloc[:,-7:]\n",
    "y= np.array(y)\n",
    "\n",
    "# X_val = np.load('./features/test_features_6.npy')\n",
    "X_test = np.load('./features/test_features_8.npy')\n",
    "X_test = np.real(X_test)\n",
    "y_test = test_csv.iloc[:,-7:]\n",
    "y_test = np.array(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28146d60",
   "metadata": {},
   "source": [
    "#### 3. 필요한 함수들 설정(모델 포함)\n",
    "- 비프음 함수\n",
    "- 음성 특징 학습 함수(CNN)\n",
    "- 문자 특징 학습 모델(KoELECTRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "817dc5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#에러 발생 혹은 작업 완료시 beep로 알리기\n",
    "def beepError():\n",
    "    fr = 1000    # range : 37 ~ 32767\n",
    "    du = 2000     # 1000 ms ==1second\n",
    "    ws.Beep(fr, du) # winsound.Beep(frequency, duration)\n",
    "\n",
    "def beepSuccess():\n",
    "    fr = 2000    # range : 37 ~ 32767\n",
    "    du = 1000     # 1000 ms ==1second\n",
    "    ws.Beep(fr, du) # winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "816a2a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 스코어를 쓸 수 있게 만드는 함수\n",
    "# f1 스코어의 결과 타입은 tensor이므로 이걸 변환해 주는 작업 필요.\n",
    "\n",
    "# class F1ScoreCallback(Callback):\n",
    "#     def __init__(self, validation_data, save_best_only=True):\n",
    "#         super(F1ScoreCallback, self).__init__()\n",
    "#         self.X_val, self.y_val = validation_data\n",
    "#         self.save_best_only = save_best_only\n",
    "#         self.best_f1 = 0\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         y_pred = np.argmax(self.model.predict(self.X_val), axis=1)\n",
    "#         f1_val = f1_score(np.argmax(self.y_val, axis=1), y_pred, average='weighted')\n",
    "#         print(f' - val_f1_score: {f1_val:.4f}')\n",
    "#         if f1_val > self.best_f1:\n",
    "#             self.best_f1 = f1_val\n",
    "#             self.model.save('./model/best_model.keras')\n",
    "#             print(\"Best F1 score. Model saved.\")\n",
    "#         elif not self.save_best_only:\n",
    "#             self.model.save('./model/best_model.keras')\n",
    "#             print(\"Model saved.\")\n",
    "#         if logs is not None:\n",
    "#             logs['val_f1_score'] = f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81dbe171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음성 모델\n",
    "def cnn_model(data):\n",
    "    model=Sequential()\n",
    "    model.add(InputLayer(shape=(data.shape[1],data.shape[2])))\n",
    "    model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "    model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "    model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=32, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(units=7, activation='softmax'))\n",
    "    model.compile(optimizer = 'adamW' , loss = 'categorical_crossentropy' , metrics = ['accuracy','f1_score']) #metrics 추가\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b6ef8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectraForMultiClassClassification(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ElectraForMultiClassClassification, self).__init__()\n",
    "        self.electra = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.electra.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.electra(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] 토큰의 표현\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a02f364",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Input(Dataset):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.data.loc[(train_csv['감정'] == \"Fear\"), '감정'] = 0\n",
    "        self.data.loc[(train_csv['감정'] == \"Surprise\"), '감정'] = 1\n",
    "        self.data.loc[(train_csv['감정'] == \"Angry\"), '감정'] = 2\n",
    "        self.data.loc[(train_csv['감정'] == \"Sadness\"), '감정'] = 3\n",
    "        self.data.loc[(train_csv['감정'] == \"Neutral\"), '감정'] = 4\n",
    "        self.data.loc[(train_csv['감정'] == \"Happiness\"), '감정'] = 5\n",
    "        self.data.loc[(train_csv['감정'] == \"Disgust\"), '감정'] = 6\n",
    "        self.tokenizer =  ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx, 1:3].values\n",
    "        text = row[0]\n",
    "        y = row[1]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            padding='max_length',\n",
    "            add_special_tokens=True\n",
    "            )\n",
    "\n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        attention_mask = inputs['attention_mask'][0]\n",
    "\n",
    "        return input_ids, attention_mask, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24769342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본적인 사전 설정\n",
    "rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=5, min_lr=0.0000001) #learning rate 조절 \n",
    "modelpath = './model/model_{epoch:02d}-{val_accuracy:.4f}.keras'\n",
    "# modelpath = './model/model_{epoch:02d}-{f1_score:.4f}.keras'\n",
    "\n",
    "\n",
    "mcp = ModelCheckpoint(\n",
    "    modelpath,     #저장할 모델의 경로\n",
    "  monitor = 'val_accuracy', #val_f1_score를 기준으로 전보다 모델이 나아지는 걸 확인\n",
    " \n",
    "  save_best_only = True,    #나아진 결과만 저장\n",
    "#     save_weights_only=True , #이걸 써 줘야 weights.h5로 저장 가능하다.\n",
    "  verbose = 1               #과정을 출력\n",
    ")\n",
    "\n",
    "#전보다 나아지지 않으면 학습중단\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor = 'val_accuracy',\n",
    "#     mode='max',\n",
    "    patience = 5,    # 전보다 나아지지 않아도 실행할 횟수\n",
    "    restore_best_weights=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ce8be0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./scaler/rscaler.pkl']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler,'./scaler/rscaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ab22743",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.3732 - f1_score: 0.1212 - loss: 1.6757\n",
      "Epoch 1: val_accuracy improved from -inf to 0.43256, saving model to ./model/model_01-0.4326.keras\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - accuracy: 0.3732 - f1_score: 0.1213 - loss: 1.6757 - val_accuracy: 0.4326 - val_f1_score: 0.2029 - val_loss: 1.5448 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4200 - f1_score: 0.1948 - loss: 1.5601\n",
      "Epoch 2: val_accuracy improved from 0.43256 to 0.44254, saving model to ./model/model_02-0.4425.keras\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - accuracy: 0.4200 - f1_score: 0.1948 - loss: 1.5601 - val_accuracy: 0.4425 - val_f1_score: 0.2139 - val_loss: 1.5122 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4448 - f1_score: 0.2154 - loss: 1.5113\n",
      "Epoch 3: val_accuracy improved from 0.44254 to 0.44923, saving model to ./model/model_03-0.4492.keras\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 15ms/step - accuracy: 0.4448 - f1_score: 0.2154 - loss: 1.5113 - val_accuracy: 0.4492 - val_f1_score: 0.2354 - val_loss: 1.4906 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m987/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4625 - f1_score: 0.2391 - loss: 1.4840\n",
      "Epoch 4: val_accuracy improved from 0.44923 to 0.44973, saving model to ./model/model_04-0.4497.keras\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 14ms/step - accuracy: 0.4625 - f1_score: 0.2390 - loss: 1.4840 - val_accuracy: 0.4497 - val_f1_score: 0.2368 - val_loss: 1.5057 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m987/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4622 - f1_score: 0.2471 - loss: 1.4638\n",
      "Epoch 5: val_accuracy improved from 0.44973 to 0.46034, saving model to ./model/model_05-0.4603.keras\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15ms/step - accuracy: 0.4623 - f1_score: 0.2471 - loss: 1.4638 - val_accuracy: 0.4603 - val_f1_score: 0.2421 - val_loss: 1.4825 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m989/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4807 - f1_score: 0.2661 - loss: 1.4328\n",
      "Epoch 6: val_accuracy did not improve from 0.46034\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15ms/step - accuracy: 0.4807 - f1_score: 0.2661 - loss: 1.4328 - val_accuracy: 0.4562 - val_f1_score: 0.2472 - val_loss: 1.4582 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m988/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4879 - f1_score: 0.2768 - loss: 1.4067\n",
      "Epoch 7: val_accuracy improved from 0.46034 to 0.46805, saving model to ./model/model_07-0.4680.keras\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15ms/step - accuracy: 0.4879 - f1_score: 0.2768 - loss: 1.4067 - val_accuracy: 0.4680 - val_f1_score: 0.2625 - val_loss: 1.4511 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m987/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4991 - f1_score: 0.2945 - loss: 1.3863\n",
      "Epoch 8: val_accuracy improved from 0.46805 to 0.46893, saving model to ./model/model_08-0.4689.keras\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15ms/step - accuracy: 0.4991 - f1_score: 0.2945 - loss: 1.3863 - val_accuracy: 0.4689 - val_f1_score: 0.2701 - val_loss: 1.4432 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m988/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5034 - f1_score: 0.3030 - loss: 1.3675\n",
      "Epoch 9: val_accuracy did not improve from 0.46893\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15ms/step - accuracy: 0.5034 - f1_score: 0.3030 - loss: 1.3675 - val_accuracy: 0.4614 - val_f1_score: 0.2655 - val_loss: 1.4816 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5197 - f1_score: 0.3267 - loss: 1.3451\n",
      "Epoch 10: val_accuracy did not improve from 0.46893\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15ms/step - accuracy: 0.5197 - f1_score: 0.3267 - loss: 1.3451 - val_accuracy: 0.4674 - val_f1_score: 0.2716 - val_loss: 1.4508 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m987/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5257 - f1_score: 0.3364 - loss: 1.3169\n",
      "Epoch 11: val_accuracy improved from 0.46893 to 0.47335, saving model to ./model/model_11-0.4734.keras\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15ms/step - accuracy: 0.5257 - f1_score: 0.3364 - loss: 1.3169 - val_accuracy: 0.4734 - val_f1_score: 0.2804 - val_loss: 1.4615 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m989/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5319 - f1_score: 0.3483 - loss: 1.2970\n",
      "Epoch 12: val_accuracy did not improve from 0.47335\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15ms/step - accuracy: 0.5319 - f1_score: 0.3482 - loss: 1.2970 - val_accuracy: 0.4680 - val_f1_score: 0.2841 - val_loss: 1.4776 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m987/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5413 - f1_score: 0.3639 - loss: 1.2777\n",
      "Epoch 13: val_accuracy did not improve from 0.47335\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - accuracy: 0.5413 - f1_score: 0.3639 - loss: 1.2777 - val_accuracy: 0.4663 - val_f1_score: 0.2883 - val_loss: 1.5249 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m988/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5540 - f1_score: 0.3855 - loss: 1.2496\n",
      "Epoch 14: val_accuracy did not improve from 0.47335\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - accuracy: 0.5539 - f1_score: 0.3854 - loss: 1.2496 - val_accuracy: 0.4718 - val_f1_score: 0.2774 - val_loss: 1.5085 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m989/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5599 - f1_score: 0.3867 - loss: 1.2152\n",
      "Epoch 15: val_accuracy did not improve from 0.47335\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15ms/step - accuracy: 0.5599 - f1_score: 0.3867 - loss: 1.2153 - val_accuracy: 0.4650 - val_f1_score: 0.2914 - val_loss: 1.5202 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5647 - f1_score: 0.3979 - loss: 1.2037\n",
      "Epoch 16: val_accuracy did not improve from 0.47335\n",
      "\u001b[1m990/990\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15ms/step - accuracy: 0.5647 - f1_score: 0.3979 - loss: 1.2037 - val_accuracy: 0.4673 - val_f1_score: 0.2893 - val_loss: 1.5221 - learning_rate: 0.0010\n",
      "\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4872 - f1_score: 0.2175 - loss: 1.4118\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4309 - f1_score: 0.2573 - loss: 1.6125\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 41\u001b[0m\n\u001b[0;32m     37\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(X_test_scaled  ,axis \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     39\u001b[0m test_loss, test_acc, test_f1_score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test_scaled, y_test)\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValid Loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m,val_loss, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValid f1_score: \u001b[39m\u001b[38;5;124m\"\u001b[39m,val_f1_score, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValid Accuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m,val_acc, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_loss' is not defined"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "best_score = 0\n",
    "best_model = None\n",
    "\n",
    "# KFold를 통한 데이터 분할 및 스케일링\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_valid = X[train_index], X[test_index]\n",
    "    y_train, y_valid = y[train_index], y[test_index]\n",
    "    \n",
    "    # 스케일링을 위한 StandardScaler 객체 생성 및 학습 데이터에 fit\n",
    "#     scaler = StandardScaler()\n",
    "#     scaler = MinMaxScaler()\n",
    "    scaler = RobustScaler()\n",
    "    scaler = scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    \n",
    "    # 학습 데이터와 동일한 스케일링을 테스트 데이터에 적용\n",
    "    X_valid_scaled = scaler.transform(X_valid)\n",
    "    \n",
    "    X_train_scaled  = np.expand_dims(X_train_scaled,axis =1)\n",
    "    X_valid_scaled = np.expand_dims(X_valid_scaled ,axis =1)\n",
    "    \n",
    "    # 모델 학습\n",
    "    model = cnn_model(X_train_scaled)\n",
    "    history=model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,batch_size=32, #32일 때 약 35% 성능, 16일 때 약 35.2% 성능, 8일 때는 약 41.4%\n",
    "    epochs=50, \n",
    "    validation_data=(X_valid_scaled , y_valid), \n",
    "    callbacks=[rlrp,mcp,es]\n",
    "    )\n",
    "    \n",
    "    # 모델 평가\n",
    "    val_loss, val_acc, val_f1_score = model.evaluate(X_valid_scaled, y_valid)\n",
    "    \n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_test_scaled = np.expand_dims(X_test_scaled  ,axis =1)\n",
    "    \n",
    "    test_loss, test_acc, test_f1_score = model.evaluate(X_test_scaled, y_test)\n",
    "    \n",
    "    print(\"Valid Loss: \",val_loss, end=', ')\n",
    "    print(\"Valid f1_score: \",val_f1_score, end=', ')\n",
    "    print(\"Valid Accuracy: \",val_acc, end=', ')\n",
    "    print(\"Test Loss: \",test_loss, end=', ')\n",
    "    print(\"Test f1_score: \",test_f1_score, end=', ')\n",
    "    print(\"Test Accuracy: \",test_acc, end=', ')\n",
    "    a\n",
    "    \n",
    "    if test_acc>best_score:\n",
    "        best_score = test_acc\n",
    "        best_model = model\n",
    "        print('더 나은 test_acc 모델 저장!')\n",
    "        \n",
    "beepSuccess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6353b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e70cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, valid_text = train_test_split(train_csv, test_size=0.3, random_state=42, shuffle=True, stratify=train_csv['감정'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fb7327",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_input = Text_Input(train_text)\n",
    "valid_text_input = Text_Input(valid_text)\n",
    "test_text_input = Text_Input(test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f9ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bface71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_text_input, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_text_input, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_text_input, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea161626",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batches = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "\n",
    "for input_ids_batch, attention_masks_batch, y_batch in tqdm(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    y_batch = y_batch.to(device)\n",
    "    input_ids_batch = input_ids_batch.to(device)\n",
    "    attention_masks_batch = attention_masks_batch.to(device)\n",
    "    y_pred = model(input_ids_batch, attention_masks_batch)\n",
    "    try:\n",
    "        loss = F.cross_entropy(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(input_ids_batch.size(), attention_masks_batch.size(), y_pred.size(), y_batch.size())\n",
    "        break\n",
    "\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "    correct += (predicted == y_batch).sum()\n",
    "    total += len(y_batch)\n",
    "\n",
    "    batches += 1\n",
    "    if batches % 100 == 0 and total != 0:\n",
    "        print(\"Batch Loss:\", total_loss, \"Accuracy:\", correct / total)\n",
    "    else:\n",
    "        accuracies.append(0)\n",
    "\n",
    "losses.append(total_loss)\n",
    "accuracies.append(correct / total)\n",
    "print(\"Train Loss:\", total_loss, \"Accuracy:\", correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92048ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749e6a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장하기\n",
    "torch.save(model.state_dict(), \"./model/1st_koElectraModel.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcd0f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "for input_ids_batch, attention_masks_batch, y_batch in tqdm(test_loader):\n",
    "    y_batch = y_batch.to(device)\n",
    "    y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "    test_correct += (predicted == y_batch).sum()\n",
    "    test_total += len(y_batch)\n",
    "\n",
    "print(\"Accuracy:\", test_correct.float() / test_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2f07b5",
   "metadata": {},
   "source": [
    "### 문제:\n",
    "\n",
    "- Colab에서 돌려본 텍스트 모델 KoELECTRA 성능이 생각 외로 형편없다....(최종 점검 결과 약 20%)\n",
    "- 한 번 더 데이터 확인해 보고 문제 없으면 다른 방법 강구해 보자\n",
    "- 그리고 앞으로는 train_test_split이나 KFold로 분리한 train data의 일부는 valid, validation으로 지칭하고 원래 분리해 놓은 세트는 test로 지칭. 용어 혼선이 있었음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d179ba89",
   "metadata": {},
   "source": [
    "# 텍스트 전처리\n",
    "- 나중에 10차 시도에 잘라 붙일 부분"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
