{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02159c55",
   "metadata": {},
   "source": [
    "#### 새로운 시도를 하기 전에 한 번 지금까지 썼던 모델들 다 검토해보기로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fd64480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "# import librosa.display\n",
    "# import IPython.display as ipd\n",
    "# from IPython.display import Audio\n",
    "\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import pydub #오디오 파일을 다루는 라이브러리 : 변환 조작 재생 분석 등\n",
    "# import gTTS # 텍스트를 음성으로 변환하는 라이브러리. 나는 당장 쓸 일 없을 듯?\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# import np_utils\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# import winsound as sd\n",
    "# import glob\n",
    "# import os\n",
    "# import json\n",
    "# import shutil\n",
    "# import sys\n",
    "# import logging\n",
    "# import unicodedata\n",
    "from shutil import copyfile\n",
    "# import warnings\n",
    "# if not sys.warnoptions:\n",
    "#     warnings.simplefilter(\"ignore\")\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler, OneHotEncoder, scale, MinMaxScaler\n",
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import joblib\n",
    "from joblib import dump, load\n",
    "from xgboost import XGBClassifier \n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint,EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense, Conv1D, MaxPooling1D, Flatten, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.utils import to_categorical\n",
    "\n",
    "# import tensorflow\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d246f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_name):\n",
    "    \n",
    "    audio, sr = librosa.load(file_name, sr=16000)\n",
    "    \n",
    "        ##Mel-spectrogram 구현\n",
    "    spectrogram = librosa.stft(audio, n_fft=512, hop_length= 256) \n",
    "    power_spectrogram = spectrogram**2\n",
    "    mel = librosa.feature.melspectrogram(S=power_spectrogram, sr=sr)\n",
    "    mel = librosa.power_to_db(np.abs(mel)**2)\n",
    "    #mfcc 구현\n",
    "    mfccs = librosa.feature.mfcc(S = mel, n_mfcc=20)\n",
    "\n",
    "    stft = np.abs(spectrogram)\n",
    "    chroma_stft = librosa.feature.chroma_stft(S=stft,hop_length=512)\n",
    "    rms = librosa.feature.rms(y=audio)\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
    "    spectral_bandwidths = librosa.feature.spectral_bandwidth(y=audio, sr=sr)\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)\n",
    "    zero_crossing_rates = librosa.feature.zero_crossing_rate(y=audio)\n",
    "    chroma_cens = librosa.feature.chroma_cens(y=audio, sr=sr)\n",
    "    tempo, _ = librosa.beat.beat_track(y=audio, sr=sr)\n",
    "    ps = perceptual_sharpness(audio)\n",
    "    \n",
    "    mfccs_mean = mfccs.mean(axis=1)\n",
    "    mfccs_var = mfccs.mean(axis=1)\n",
    "    \n",
    "    for i in range(len(mfccs_mean)):\n",
    "        locals()[f'mfccs_mean_{i}'] = mfccs_mean[i]\n",
    "        locals()[f'mfccs_var_{i}'] = mfccs_var[i]\n",
    "    chroma_stft_mean = chroma_stft.mean()\n",
    "    chroma_stft_var = chroma_stft.var()\n",
    "    rms_mean = rms.mean()\n",
    "    rms_var = rms.var()\n",
    "    spectral_centroids_mean = spectral_centroids.mean()\n",
    "    spectral_centroids_var = spectral_centroids.var()\n",
    "    spectral_bandwidths_mean = spectral_bandwidths.mean()\n",
    "    spectral_bandwidths_var = spectral_bandwidths.var()\n",
    "    spectral_rolloff_mean = spectral_rolloff.mean()\n",
    "    spectral_rolloff_var = spectral_rolloff.var()\n",
    "    zero_crossing_rates_mean = zero_crossing_rates.mean()\n",
    "    zero_crossing_rates_var = zero_crossing_rates.var()\n",
    "    harmony_mean = chroma_cens.mean()\n",
    "    harmony_var = chroma_cens.var()\n",
    "    tempo_mean = tempo.mean()\n",
    "    tempo_var = tempo.var()\n",
    "    perceptual_sharpness_mean = ps.mean()\n",
    "    perceptual_sharpness_var = ps.var()\n",
    "    \n",
    "    #합치기\n",
    "    features = np.array([])\n",
    "    for j in range(20):\n",
    "        features = np.hstack((features,locals()[f'mfccs_mean_{j}'],locals()[f'mfccs_var_{j}']))\n",
    "    \n",
    "    features = np.hstack((features,chroma_stft_mean,chroma_stft_var,rms_mean,rms_var,spectral_centroids_mean\n",
    "                         ,spectral_centroids_var, spectral_bandwidths_mean, spectral_bandwidths_var,spectral_rolloff_mean\n",
    "                         , spectral_rolloff_var,zero_crossing_rates_mean, zero_crossing_rates_var, harmony_mean, harmony_var\n",
    "                         , tempo_mean,tempo_var,perceptual_sharpness_mean,perceptual_sharpness_var))\n",
    "    features = scale(features)\n",
    "#     features = np.pad(features,(2,2),mode='constant')\n",
    "    \n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc9390bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('e:/Data2/csv/train.csv')\n",
    "valid_csv = pd.read_csv('e:/Data2/csv/valid.csv')\n",
    "X = np.load('./features5.npy')\n",
    "y = train_csv.iloc[:,-7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41ebe899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본적인 사전 설정\n",
    "rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=5, min_lr=0.0000001) #learning rate 조절 \n",
    "modelpath = './model/model_{epoch:02d}-{val_accuracy:.4f}.keras'\n",
    "mcp = ModelCheckpoint(\n",
    "    modelpath,     #저장할 모델의 경로\n",
    "  monitor = 'val_f1score', #val_acc를 기준으로 전보다 모델이 나아지는 걸 확인\n",
    "  save_best_only = True,    #나아진 결과만 저장\n",
    "#     save_weights_only=True , #이걸 써 줘야 weights.h5로 저장 가능하다.\n",
    "  verbose = 1               #과정을 출력\n",
    ")\n",
    "\n",
    "#전보다 나아지지 않으면 학습중단\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor = 'val_accuracy',\n",
    "    patience = 5      # 전보다 나아지지 않아도 실행할 횟수\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5648cd85",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9478c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(x_train):\n",
    "    model=Sequential()\n",
    "    model.add(InputLayer(shape=(x_train.shape[1],x_train.shape[2])))\n",
    "    model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "    model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "    model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=32, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(units=7, activation='softmax'))\n",
    "    model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy',f1score])\n",
    "\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eafb9582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m506/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3592 - loss: 1.6858\n",
      "Epoch 1: val_accuracy did not improve from 0.50614\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.3598 - loss: 1.6847 - val_accuracy: 0.4484 - val_loss: 1.5184 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m511/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4484 - loss: 1.5211\n",
      "Epoch 2: val_accuracy did not improve from 0.50614\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.4484 - loss: 1.5210 - val_accuracy: 0.4511 - val_loss: 1.4871 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4584 - loss: 1.4783\n",
      "Epoch 3: val_accuracy did not improve from 0.50614\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.4584 - loss: 1.4783 - val_accuracy: 0.4679 - val_loss: 1.4360 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m511/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4747 - loss: 1.4391\n",
      "Epoch 4: val_accuracy did not improve from 0.50614\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.4747 - loss: 1.4391 - val_accuracy: 0.4754 - val_loss: 1.4141 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m506/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4845 - loss: 1.4210\n",
      "Epoch 5: val_accuracy did not improve from 0.50614\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.4845 - loss: 1.4209 - val_accuracy: 0.4750 - val_loss: 1.4300 - learning_rate: 0.0010\n",
      "Test Accuracy:  0.4749886393547058\n"
     ]
    }
   ],
   "source": [
    "#StandardScaler로 \n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "   \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0,  test_size = 0.3, stratify = y)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = np.expand_dims(X_train_scaled,axis =1)\n",
    "X_test_scaled = np.expand_dims(X_test_scaled,axis =1)\n",
    "\n",
    "\n",
    "model = second_model(X_train_scaled)\n",
    "history=model.fit(X_train_scaled, y_train, batch_size=40, epochs=50, validation_data=(X_test_scaled, y_test), callbacks=[rlrp,mcp,es])\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(\"Test Accuracy: \",test_acc)\n",
    "\n",
    "# test 결과값 약 47.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91685782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m508/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3723 - loss: 1.7180\n",
      "Epoch 1: val_accuracy did not improve from 0.50614\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.3725 - loss: 1.7175 - val_accuracy: 0.3832 - val_loss: 1.6285 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m513/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3932 - loss: 1.6417\n",
      "Epoch 2: val_accuracy did not improve from 0.50614\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.3932 - loss: 1.6417 - val_accuracy: 0.3830 - val_loss: 1.6234 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m513/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3923 - loss: 1.6229\n",
      "Epoch 3: val_accuracy did not improve from 0.50614\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.3923 - loss: 1.6229 - val_accuracy: 0.4051 - val_loss: 1.5886 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m506/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4002 - loss: 1.5998\n",
      "Epoch 4: val_accuracy did not improve from 0.50614\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.4003 - loss: 1.5998 - val_accuracy: 0.4084 - val_loss: 1.5726 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4009 - loss: 1.6033\n",
      "Epoch 5: val_accuracy did not improve from 0.50614\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.4009 - loss: 1.6033 - val_accuracy: 0.4126 - val_loss: 1.5696 - learning_rate: 0.0010\n",
      "Test Accuracy:  0.412573903799057\n"
     ]
    }
   ],
   "source": [
    "#MinMaxScaler로 \n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "   \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0,  test_size = 0.3, stratify = y)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = np.expand_dims(X_train_scaled,axis =1)\n",
    "X_test_scaled = np.expand_dims(X_test_scaled,axis =1)\n",
    "\n",
    "\n",
    "model = second_model(X_train_scaled)\n",
    "history=model.fit(X_train_scaled, y_train, batch_size=40, epochs=50, validation_data=(X_test_scaled, y_test), callbacks=[rlrp,mcp,es])\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(\"Test Accuracy: \",test_acc)\n",
    "\n",
    "# test 결과값 약 41.2% : CNN은 StandardScaler로"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff52221",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "705fc471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train KNN Accuracy: 0.4249378745797398\n",
      "Test KNN Accuracy: 0.3358344702137335\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for the random search\n",
    "X_train, X_test, y_train, y_test = train_test_split(X , train_csv['감정'],random_state=0, stratify = train_csv['감정'], test_size = 0.3)\n",
    "\n",
    "\n",
    "\n",
    "# 레이블 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': np.arange(1, 15),  # Number of neighbors\n",
    "    'weights': ['uniform', 'distance'],  # Weight function\n",
    "    'p': [1, 2]  # Power parameter for the Minkowski distance metric\n",
    "}\n",
    "\n",
    "# Create the KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform the random search\n",
    "random_search_knn = RandomizedSearchCV(\n",
    "    knn, param_distributions=param_grid, n_iter=10, cv=5, random_state=42\n",
    ")\n",
    "random_search_knn.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Evaluate the KNN model with the best parameters on the test set\n",
    "best_knn = random_search_knn.best_estimator_\n",
    "y_pred_knn = best_knn.predict(X_test)\n",
    "test_accuracy_knn = accuracy_score(y_test_encoded, y_pred_knn)\n",
    "\n",
    "# Evaluate the KNN model on the training set\n",
    "y_train_pred_knn = best_knn.predict(X_train)\n",
    "train_accuracy_knn = accuracy_score(y_train_encoded, y_train_pred_knn)\n",
    "\n",
    "print(\"Train KNN Accuracy:\", train_accuracy_knn)\n",
    "print(\"Test KNN Accuracy:\", test_accuracy_knn)\n",
    "\n",
    "\n",
    "#test 결과값 약 33% 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c98f5c1",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52a074a1",
   "metadata": {},
   "outputs": [],
   "source": [
    " def lstm_model(X_train):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(shape=(X_train.shape[1],X_train.shape[2])))\n",
    "    model.add(LSTM(64, return_sequences=True)) \n",
    "    model.add(LSTM(32)) \n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c1c51aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m455/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3301 - loss: 0.4530\n",
      "Epoch 1: val_accuracy did not improve from 0.81958\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.3327 - loss: 0.4493 - val_accuracy: 0.4216 - val_loss: 0.3428 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m440/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4317 - loss: 0.3357\n",
      "Epoch 2: val_accuracy did not improve from 0.81958\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4325 - loss: 0.3354 - val_accuracy: 0.4530 - val_loss: 0.3296 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m446/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4613 - loss: 0.3242\n",
      "Epoch 3: val_accuracy did not improve from 0.81958\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4614 - loss: 0.3241 - val_accuracy: 0.4665 - val_loss: 0.3237 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m461/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4747 - loss: 0.3162\n",
      "Epoch 4: val_accuracy did not improve from 0.81958\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4748 - loss: 0.3162 - val_accuracy: 0.4694 - val_loss: 0.3219 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m442/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4873 - loss: 0.3117\n",
      "Epoch 5: val_accuracy did not improve from 0.81958\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4877 - loss: 0.3116 - val_accuracy: 0.4726 - val_loss: 0.3185 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, stratify = y, test_size = 0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = np.expand_dims(X_train_scaled,axis =1)\n",
    "X_test_scaled = np.expand_dims(X_test_scaled,axis =1)\n",
    "\n",
    "model = lstm_model(X_train_scaled)\n",
    "\n",
    "h1 = model.fit(X_train_scaled,y_train,\n",
    "               validation_split = 0.3,\n",
    "               epochs=30,\n",
    "               batch_size=30,callbacks=[rlrp,es,mcp])\n",
    "\n",
    "#test 결과값 약 46.8% 정도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4be48111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m447/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4046 - loss: 0.3499\n",
      "Epoch 1: val_accuracy did not improve from 0.50807\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4044 - loss: 0.3498 - val_accuracy: 0.4174 - val_loss: 0.3453 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m454/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4147 - loss: 0.3441\n",
      "Epoch 2: val_accuracy did not improve from 0.50807\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4148 - loss: 0.3440 - val_accuracy: 0.4096 - val_loss: 0.3454 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4193 - loss: 0.3422\n",
      "Epoch 3: val_accuracy did not improve from 0.50807\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4194 - loss: 0.3421 - val_accuracy: 0.4328 - val_loss: 0.3396 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m462/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4339 - loss: 0.3357\n",
      "Epoch 4: val_accuracy did not improve from 0.50807\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4341 - loss: 0.3357 - val_accuracy: 0.4410 - val_loss: 0.3343 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m448/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4457 - loss: 0.3342\n",
      "Epoch 5: val_accuracy did not improve from 0.50807\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4456 - loss: 0.3341 - val_accuracy: 0.4296 - val_loss: 0.3359 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, stratify = y, test_size = 0.3)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = np.expand_dims(X_train_scaled,axis =1)\n",
    "X_test_scaled = np.expand_dims(X_test_scaled,axis =1)\n",
    "\n",
    "model = lstm_model(X_train_scaled)\n",
    "\n",
    "h1 = model.fit(X_train_scaled,y_train,\n",
    "               validation_split = 0.3,\n",
    "               epochs=30,\n",
    "               batch_size=30,callbacks=[rlrp,es,mcp])\n",
    "\n",
    "#test 결과값 약 42.7% 정도. LSTM은 StandardScaler로"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0018387",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "672f4999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 : 0.5160300136425648\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train,y_test =train_test_split(X,train_csv['감정'],random_state=5,stratify=train_csv['감정'],test_size=0.3)\n",
    "#MinMAx Scaler을 이용한 특징 벡터 전처리\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 레이블 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "\n",
    "#분류기 커널 설정\n",
    "clf = SVC(C=100, kernel='rbf', probability=True)\n",
    "#분류기 학습\n",
    "clf.fit(X_train_scaled, y_train_encoded )\n",
    "#각 row의 클래스별 확률을 구하기\n",
    "probabilities = clf.predict_proba(X_test_scaled)\n",
    "print(\"첫 번째 샘플의 클래스별 확률:\", probabilities[0])\n",
    "#예측 결과\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_pred, y_test_encoded)\n",
    "    \n",
    "\n",
    "    \n",
    "print(\"정확도 :\", accuracy)\n",
    "\n",
    "#test 결과값 약 51.6% 정도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc069824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 샘플의 클래스별 확률: [0.20003447 0.10426802 0.08509696 0.04807575 0.17558152 0.35403075\n",
      " 0.03291253]\n",
      "정확도 : 0.47510231923601637\n"
     ]
    }
   ],
   "source": [
    "#Scaler를 StandardScaler로\n",
    "\n",
    "X_train, X_test, y_train,y_test =train_test_split(X,train_csv['감정'],random_state=5,stratify=train_csv['감정'],test_size=0.3)\n",
    "#StandardScaler을 이용한 특징 벡터 전처리\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 레이블 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "\n",
    "#분류기 커널 설정\n",
    "clf = SVC(C=100, kernel='rbf', probability=True)\n",
    "#분류기 학습\n",
    "clf.fit(X_train_scaled, y_train_encoded )\n",
    "#각 row의 클래스별 확률을 구하기\n",
    "probabilities = clf.predict_proba(X_test_scaled)\n",
    "print(\"첫 번째 샘플의 클래스별 확률:\", probabilities[0])\n",
    "#예측 결과\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_pred, y_test_encoded)\n",
    "    \n",
    "\n",
    "    \n",
    "print(\"정확도 :\", accuracy)\n",
    "\n",
    "#test 결과값 약 47.5% 정도. SVC에서는 MinMaxScaler로"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f655803",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "877a0a14",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smhrd\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m623/642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3924 - loss: 1.6399\n",
      "Epoch 1: val_accuracy did not improve from 0.50807\n",
      "\u001b[1m642/642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.3932 - loss: 1.6378 - val_accuracy: 0.4578 - val_loss: 1.4724 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m616/642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4463 - loss: 1.4941\n",
      "Epoch 2: val_accuracy did not improve from 0.50807\n",
      "\u001b[1m642/642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4465 - loss: 1.4937 - val_accuracy: 0.4773 - val_loss: 1.4357 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m630/642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4760 - loss: 1.4297\n",
      "Epoch 3: val_accuracy did not improve from 0.50807\n",
      "\u001b[1m642/642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4758 - loss: 1.4300 - val_accuracy: 0.4717 - val_loss: 1.4341 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m641/642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4723 - loss: 1.4235\n",
      "Epoch 4: val_accuracy did not improve from 0.50807\n",
      "\u001b[1m642/642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4723 - loss: 1.4234 - val_accuracy: 0.4765 - val_loss: 1.4274 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m611/642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4825 - loss: 1.3928\n",
      "Epoch 5: val_accuracy did not improve from 0.50807\n",
      "\u001b[1m642/642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4823 - loss: 1.3934 - val_accuracy: 0.4724 - val_loss: 1.4182 - learning_rate: 0.0010\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 853us/step\n",
      "Training accuracy: 0.4784388244152069\n",
      "Validation accuracy: 0.4723738133907318\n",
      "Test accuracy: 0.3993860845839018\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=5,stratify=y,test_size=0.3)\n",
    "\n",
    "scaler =StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(units=256, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=7, activation='softmax'))  # Adjusted to use np.unique for flexibility\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=1, validation_data=(X_test_scaled, y_test),callbacks=[rlrp,mcp,es])\n",
    "\n",
    "# Retrieve training and validation accuracy\n",
    "train_accuracy = history.history['accuracy'][-1]  # Last epoch accuracy\n",
    "val_accuracy = history.history['val_accuracy'][-1]  # Last epoch validation accuracy\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "# 임계값 설정\n",
    "threshold = 0.3\n",
    "\n",
    "# 확률이 임계값보다 크면 1로, 작으면 0으로 분류\n",
    "y_pred_labels = (y_pred > threshold).astype(int)\n",
    "# y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "\n",
    "# Print accuracies\n",
    "print('Training accuracy:', train_accuracy)\n",
    "print('Validation accuracy:', val_accuracy)\n",
    "print('Test accuracy:', test_accuracy)\n",
    "\n",
    "#test 결과값 약 39.9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbb85122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smhrd\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m640/642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3816 - loss: 1.6601\n",
      "Epoch 1: val_accuracy did not improve from 0.50807\n",
      "\u001b[1m642/642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.3816 - loss: 1.6600 - val_accuracy: 0.4048 - val_loss: 1.6065 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m629/642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4023 - loss: 1.5928\n",
      "Epoch 2: val_accuracy did not improve from 0.50807\n",
      "\u001b[1m642/642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4023 - loss: 1.5928 - val_accuracy: 0.3906 - val_loss: 1.6358 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m625/642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4080 - loss: 1.5766\n",
      "Epoch 3: val_accuracy did not improve from 0.50807\n",
      "\u001b[1m642/642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4081 - loss: 1.5765 - val_accuracy: 0.4173 - val_loss: 1.5588 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m626/642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4138 - loss: 1.5649\n",
      "Epoch 4: val_accuracy did not improve from 0.50807\n",
      "\u001b[1m642/642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4139 - loss: 1.5647 - val_accuracy: 0.4361 - val_loss: 1.5239 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m628/642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4236 - loss: 1.5524\n",
      "Epoch 5: val_accuracy did not improve from 0.50807\n",
      "\u001b[1m642/642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4236 - loss: 1.5522 - val_accuracy: 0.4212 - val_loss: 1.5416 - learning_rate: 0.0010\n",
      "\u001b[1m275/275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892us/step\n",
      "Training accuracy: 0.4235248267650604\n",
      "Validation accuracy: 0.42121419310569763\n",
      "Test accuracy: 0.3848340154615734\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=5,stratify=y,test_size=0.3)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(units=256, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=7, activation='softmax'))  # Adjusted to use np.unique for flexibility\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy',])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=1, validation_data=(X_test_scaled, y_test),callbacks=[rlrp,mcp,es])\n",
    "\n",
    "# Retrieve training and validation accuracy\n",
    "train_accuracy = history.history['accuracy'][-1]  # Last epoch accuracy\n",
    "val_accuracy = history.history['val_accuracy'][-1]  # Last epoch validation accuracy\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "# 임계값 설정\n",
    "threshold = 0.3\n",
    "\n",
    "# 확률이 임계값보다 크면 1로, 작으면 0으로 분류\n",
    "y_pred_labels = (y_pred > threshold).astype(int)\n",
    "# y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "\n",
    "# Print accuracies\n",
    "print('Training accuracy:', train_accuracy)\n",
    "print('Validation accuracy:', val_accuracy)\n",
    "print('Test accuracy:', test_accuracy)\n",
    "\n",
    "#test 결과값 약 38.4% 큰 차이는 아니지만 StandardScaler 성능이 더 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f30fb77",
   "metadata": {},
   "source": [
    "### Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc008f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.29\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=5,stratify=y,test_size=0.3)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=1000, learning_rate=0.05) #1000개의 가지? epoch? , 0.05 학습률\n",
    "xgb.fit(X_train_scaled, y_train) #학습\n",
    "\n",
    "y_preds = xgb.predict(X_test_scaled) #검증\n",
    "\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test,y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa89095d",
   "metadata": {},
   "source": [
    "### 확인 결과\n",
    "- CNN, LSTM, SVC의 성능이 괜찮은 것으로 보임\n",
    "- 이제 이걸 텍스트 모델과 결합하는 형태로 실행해 보자.\n",
    "- CNN은 StandardScaler,LSTM은 StandardScaler,SVC에서는 MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46b4c42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 함수 설정\n",
    "class text_embedding():\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "  \n",
    "    def transform(self, X,raw_label):\n",
    "        embedding_model = SentenceTransformer(self.model_name)\n",
    "        embedding_vec = embedding_model.encode(raw_label)\n",
    "        X_val = np.concatenate((X, embedding_vec), axis = 1)\n",
    "        return X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "74b720d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler, 'scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cf11ab",
   "metadata": {},
   "source": [
    "### CNN 모델(텍스트 임베딩)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bbc19a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smhrd\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 20ms/step - accuracy: 0.5418 - loss: 1.2261 - val_accuracy: 0.7149 - val_loss: 0.7967 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m  7/514\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.7283 - loss: 0.7496 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smhrd\\anaconda3\\Lib\\site-packages\\keras\\src\\callbacks\\model_checkpoint.py:206: UserWarning: Can save best model only with val_f1score available, skipping.\n",
      "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.7308 - loss: 0.7831 - val_accuracy: 0.7379 - val_loss: 0.7204 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.7489 - loss: 0.7219 - val_accuracy: 0.7589 - val_loss: 0.6925 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.7761 - loss: 0.6296 - val_accuracy: 0.7624 - val_loss: 0.6825 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.7843 - loss: 0.6050 - val_accuracy: 0.7627 - val_loss: 0.6923 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.7992 - loss: 0.5662 - val_accuracy: 0.7669 - val_loss: 0.6786 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.7989 - loss: 0.5618 - val_accuracy: 0.7724 - val_loss: 0.6696 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.8177 - loss: 0.5019 - val_accuracy: 0.7752 - val_loss: 0.6791 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.8267 - loss: 0.4875 - val_accuracy: 0.7749 - val_loss: 0.6783 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.8262 - loss: 0.4672 - val_accuracy: 0.7725 - val_loss: 0.7227 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - accuracy: 0.8358 - loss: 0.4533 - val_accuracy: 0.7740 - val_loss: 0.7044 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.8427 - loss: 0.4267 - val_accuracy: 0.7681 - val_loss: 0.7173 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.8466 - loss: 0.4114 - val_accuracy: 0.7759 - val_loss: 0.7356 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.8549 - loss: 0.3898 - val_accuracy: 0.7704 - val_loss: 0.7719 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.8585 - loss: 0.3813 - val_accuracy: 0.7743 - val_loss: 0.8021 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.8604 - loss: 0.3771 - val_accuracy: 0.7692 - val_loss: 0.8265 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.8660 - loss: 0.3500 - val_accuracy: 0.7730 - val_loss: 0.8569 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.8822 - loss: 0.3248 - val_accuracy: 0.7705 - val_loss: 0.8839 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.8737 - loss: 0.3376 - val_accuracy: 0.7669 - val_loss: 0.8359 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.8893 - loss: 0.3067 - val_accuracy: 0.7691 - val_loss: 0.9108 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.8848 - loss: 0.3061 - val_accuracy: 0.7698 - val_loss: 0.9455 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.8889 - loss: 0.2895 - val_accuracy: 0.7685 - val_loss: 0.9653 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.8911 - loss: 0.2909 - val_accuracy: 0.7678 - val_loss: 0.9995 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9046 - loss: 0.2646 - val_accuracy: 0.7719 - val_loss: 1.0823 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9074 - loss: 0.2610 - val_accuracy: 0.7722 - val_loss: 1.0352 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9139 - loss: 0.2304 - val_accuracy: 0.7680 - val_loss: 1.1509 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9102 - loss: 0.2390 - val_accuracy: 0.7628 - val_loss: 1.1511 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 20ms/step - accuracy: 0.9115 - loss: 0.2423 - val_accuracy: 0.7741 - val_loss: 1.2012 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - accuracy: 0.9174 - loss: 0.2330 - val_accuracy: 0.7666 - val_loss: 1.2992 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.9176 - loss: 0.2219 - val_accuracy: 0.7675 - val_loss: 1.2569 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.9151 - loss: 0.2254 - val_accuracy: 0.7697 - val_loss: 1.2276 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9200 - loss: 0.2174 - val_accuracy: 0.7661 - val_loss: 1.2642 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 20ms/step - accuracy: 0.9225 - loss: 0.2146 - val_accuracy: 0.7675 - val_loss: 1.2770 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9272 - loss: 0.1917 - val_accuracy: 0.7624 - val_loss: 1.3690 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9286 - loss: 0.1987 - val_accuracy: 0.7640 - val_loss: 1.3516 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.9275 - loss: 0.1958 - val_accuracy: 0.7693 - val_loss: 1.2644 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.9303 - loss: 0.1925 - val_accuracy: 0.7711 - val_loss: 1.4157 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9314 - loss: 0.1849 - val_accuracy: 0.7665 - val_loss: 1.4345 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.9337 - loss: 0.1778 - val_accuracy: 0.7692 - val_loss: 1.4473 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.9312 - loss: 0.2017 - val_accuracy: 0.7692 - val_loss: 1.2020 - learning_rate: 0.0010\n",
      "Epoch 41/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.9464 - loss: 0.1520 - val_accuracy: 0.7684 - val_loss: 1.4033 - learning_rate: 0.0010\n",
      "Epoch 42/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9366 - loss: 0.1753 - val_accuracy: 0.7657 - val_loss: 1.4847 - learning_rate: 0.0010\n",
      "Epoch 43/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 22ms/step - accuracy: 0.9339 - loss: 0.1794 - val_accuracy: 0.7667 - val_loss: 1.6602 - learning_rate: 0.0010\n",
      "Epoch 44/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.9429 - loss: 0.1505 - val_accuracy: 0.7651 - val_loss: 1.5198 - learning_rate: 0.0010\n",
      "Epoch 45/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 21ms/step - accuracy: 0.9431 - loss: 0.1618 - val_accuracy: 0.7652 - val_loss: 1.3945 - learning_rate: 0.0010\n",
      "Epoch 46/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9414 - loss: 0.1621 - val_accuracy: 0.7660 - val_loss: 1.5365 - learning_rate: 0.0010\n",
      "Epoch 47/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9394 - loss: 0.1664 - val_accuracy: 0.7646 - val_loss: 1.5592 - learning_rate: 0.0010\n",
      "Epoch 48/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9437 - loss: 0.1545 - val_accuracy: 0.7682 - val_loss: 1.6282 - learning_rate: 0.0010\n",
      "Epoch 49/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9482 - loss: 0.1609 - val_accuracy: 0.7685 - val_loss: 1.5064 - learning_rate: 0.0010\n",
      "Epoch 50/50\n",
      "\u001b[1m514/514\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9594 - loss: 0.1079 - val_accuracy: 0.7771 - val_loss: 1.8510 - learning_rate: 4.0000e-04\n",
      "Pre-trained Model:  sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens\n",
      "Test Accuracy:  0.7770577669143677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smhrd\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[162], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m pre_trained_models:\n\u001b[0;32m     15\u001b[0m     train_txt_embed \u001b[38;5;241m=\u001b[39m text_embedding(model_name \u001b[38;5;241m=\u001b[39m i)\n\u001b[1;32m---> 16\u001b[0m     X_train_embed \u001b[38;5;241m=\u001b[39m train_txt_embed\u001b[38;5;241m.\u001b[39mtransform(X,train_csv[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m발화문\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     18\u001b[0m     X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_train_embed,y, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,  test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m, stratify \u001b[38;5;241m=\u001b[39m y)\n\u001b[0;32m     20\u001b[0m     X_train_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\n",
      "Cell \u001b[1;32mIn[82], line 11\u001b[0m, in \u001b[0;36mtext_embedding.transform\u001b[1;34m(self, X, raw_label)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X,raw_label):\n\u001b[0;32m     10\u001b[0m     embedding_model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name)\n\u001b[1;32m---> 11\u001b[0m     embedding_vec \u001b[38;5;241m=\u001b[39m embedding_model\u001b[38;5;241m.\u001b[39mencode(raw_label)\n\u001b[0;32m     12\u001b[0m     X_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((X, embedding_vec), axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_val\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:371\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    368\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 371\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(features)\n\u001b[0;32m    372\u001b[0m     out_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m truncate_embeddings(\n\u001b[0;32m    373\u001b[0m         out_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncate_dim\n\u001b[0;32m    374\u001b[0m     )\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:98\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m     96\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 98\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     99\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    101\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:814\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    812\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(input_shape, device\u001b[38;5;241m=\u001b[39mdevice)  \u001b[38;5;66;03m# (bs, seq_length)\u001b[39;00m\n\u001b[1;32m--> 814\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m    815\u001b[0m     x\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[0;32m    816\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    817\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    818\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    819\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    820\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    821\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:575\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    567\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    568\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    569\u001b[0m         hidden_state,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    572\u001b[0m         output_attentions,\n\u001b[0;32m    573\u001b[0m     )\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 575\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    576\u001b[0m         hidden_state,\n\u001b[0;32m    577\u001b[0m         attn_mask,\n\u001b[0;32m    578\u001b[0m         head_mask[i],\n\u001b[0;32m    579\u001b[0m         output_attentions,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[0;32m    582\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:501\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;124;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;124;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[1;32m--> 501\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m    502\u001b[0m     query\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    503\u001b[0m     key\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    504\u001b[0m     value\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    505\u001b[0m     mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[0;32m    506\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    507\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    508\u001b[0m )\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    510\u001b[0m     sa_output, sa_weights \u001b[38;5;241m=\u001b[39m sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:227\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[1;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(bs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads \u001b[38;5;241m*\u001b[39m dim_per_head)\n\u001b[0;32m    226\u001b[0m q \u001b[38;5;241m=\u001b[39m shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_lin(query))  \u001b[38;5;66;03m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m k \u001b[38;5;241m=\u001b[39m shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_lin(key))  \u001b[38;5;66;03m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m    228\u001b[0m v \u001b[38;5;241m=\u001b[39m shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_lin(value))  \u001b[38;5;66;03m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m    230\u001b[0m q \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(dim_per_head)  \u001b[38;5;66;03m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#StandardScaler로 \n",
    "\n",
    "pre_trained_models = [\n",
    "    'sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens',\n",
    "'sentence-transformers/multi-qa-distilbert-cos-v1',\n",
    "'jhgan/ko-sroberta-multitask',\n",
    "'all-distilroberta-v1',\n",
    "'jhgan/ko-sbert-multitask',\n",
    "'all-MiniLM-L12-v2', 'jhgan/ko-sroberta-sts'\n",
    "]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for i in pre_trained_models:\n",
    "    train_txt_embed = text_embedding(model_name = i)\n",
    "    X_train_embed = train_txt_embed.transform(X,train_csv['발화문'])\n",
    "   \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train_embed,y, random_state=0,  test_size = 0.3, stratify = y)\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    X_train_scaled = np.expand_dims(X_train_scaled,axis =1)\n",
    "    X_test_scaled = np.expand_dims(X_test_scaled,axis =1)\n",
    "\n",
    "\n",
    "    model = second_model(X_train_scaled)\n",
    "    history=model.fit(X_train_scaled, y_train,batch_size=40, epochs=50, validation_data=(X_test_scaled, y_test), callbacks=[rlrp,mcp])\n",
    "\n",
    "    \n",
    "    test_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "    print(\"Pre-trained Model: \", i)\n",
    "    print(\"Test Accuracy: \",test_acc)\n",
    "    \n",
    "#결과 \n",
    "#'sentence-transformers/multi-qa-distilbert-cos-v1'로 임베딩한 게 가장 좋다.(약 81.96%)\n",
    "# 'jhgan/ko-sbert-multitask'과 'jhgan/ko-sroberta-sts'도 나름대로 준수\n",
    "# 모델명 : model_06-0.8196.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e1988d",
   "metadata": {},
   "source": [
    "### LSTM 모델(텍스트 임베딩)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f33f2039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29319, 826)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74e345e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m473/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4199 - loss: 0.4096\n",
      "Epoch 1: val_accuracy did not improve from 0.81958\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.4209 - loss: 0.4085 - val_accuracy: 0.5774 - val_loss: 0.2692 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6253 - loss: 0.2450\n",
      "Epoch 2: val_accuracy did not improve from 0.81958\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6252 - loss: 0.2450 - val_accuracy: 0.6133 - val_loss: 0.2470 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m458/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6828 - loss: 0.2110\n",
      "Epoch 3: val_accuracy did not improve from 0.81958\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6826 - loss: 0.2111 - val_accuracy: 0.6182 - val_loss: 0.2405 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m471/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7158 - loss: 0.1911\n",
      "Epoch 4: val_accuracy did not improve from 0.81958\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7157 - loss: 0.1911 - val_accuracy: 0.6352 - val_loss: 0.2400 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m478/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7549 - loss: 0.1688\n",
      "Epoch 5: val_accuracy did not improve from 0.81958\n",
      "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7549 - loss: 0.1688 - val_accuracy: 0.6251 - val_loss: 0.2467 - learning_rate: 0.0010\n",
      "Pre-trained Model:  jhgan/ko-sroberta-sts\n",
      "Test Accuracy:  0.6230104565620422\n"
     ]
    }
   ],
   "source": [
    "txt_embed = text_embedding(model_name = 'sentence-transformers/multi-qa-distilbert-cos-v1')\n",
    "X_embed = txt_embed.transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_embed, y, random_state=0, stratify = y, test_size = 0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = np.expand_dims(X_train_scaled,axis =1)\n",
    "X_test_scaled = np.expand_dims(X_test_scaled,axis =1)\n",
    "\n",
    "model = lstm_model(X_train_scaled)\n",
    "\n",
    "history = model.fit(X_train_scaled,y_train,\n",
    "               validation_split = 0.3,\n",
    "               epochs=30,\n",
    "               batch_size=30,callbacks=[rlrp,mcp,es])\n",
    "\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(\"Pre-trained Model: \", i)\n",
    "print(\"Test Accuracy: \",test_acc)\n",
    "\n",
    "#63퍼가 최대"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a76495",
   "metadata": {},
   "source": [
    "### SVC(텍스트 임베딩)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f80ec4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 샘플의 클래스별 확률: [0.04770128 0.00425071 0.5767003  0.00182524 0.1100945  0.22532101\n",
      " 0.03410696]\n",
      "정확도 : 0.7137335152341974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_model.pkl']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_embed = text_embedding(model_name = 'sentence-transformers/multi-qa-distilbert-cos-v1') \n",
    "X_embed = txt_embed.transform(X)# 이것도\n",
    "\n",
    "X_train, X_test, y_train,y_test =train_test_split(X_embed,train_csv['감정'],random_state=5,stratify=train_csv['감정'],test_size=0.3)\n",
    "#MinMAx Scaler을 이용한 특징 벡터 전처리\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 레이블 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "\n",
    "#분류기 커널 설정\n",
    "clf = SVC(C=100, kernel='rbf', probability=True)\n",
    "#분류기 학습\n",
    "clf.fit(X_train_scaled, y_train_encoded )\n",
    "#각 row의 클래스별 확률을 구하기\n",
    "probabilities = clf.predict_proba(X_test_scaled)\n",
    "print(\"첫 번째 샘플의 클래스별 확률:\", probabilities[0])\n",
    "#예측 결과\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_pred, y_test_encoded)\n",
    "    \n",
    "\n",
    "    \n",
    "print(\"정확도 :\", accuracy)\n",
    "\n",
    "\n",
    "joblib.dump(clf, 'svm_model.pkl')\n",
    "\n",
    "# 첫 번째 샘플의 클래스별 확률: [0.04770128 0.00425071 0.5767003  0.00182524 0.1100945  0.22532101\n",
    "#  0.03410696]\n",
    "# 정확도 : 0.7137335152341974\n",
    "# ['svm_model.pkl']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a5b996",
   "metadata": {},
   "source": [
    "## 결과 \n",
    "- 위에서 나온 model_06-0.8196.keras 모델 이용해서 앞으로 작업할 것\n",
    "- Valid 데이터도 사용해서 검증해 보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0bdd50c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#사용할 모델 불러오기\n",
    "model = keras.models.load_model('./model/model_01-0.8050.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ddcd96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid에 대한 features를 불러오기\n",
    "features=np.load('./valid_features.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7c45fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 오래 걸리는 작업 그냥 저장해 놓기\n",
    "np.save('./X_valid_embed',X_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2860ebd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7206 - loss: 11.9040\n",
      "Pre-trained Model:  jhgan/ko-sroberta-multitask\n",
      "Test Accuracy:  0.5662190914154053\n"
     ]
    }
   ],
   "source": [
    "#valid data에 대해서도 검증!\n",
    "\n",
    "pre_trained_models = [\n",
    "#     'sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens',\n",
    "# 'sentence-transformers/multi-qa-distilbert-cos-v1',\n",
    "'jhgan/ko-sroberta-multitask',\n",
    "# 'all-distilroberta-v1',\n",
    "# 'jhgan/ko-sbert-multitask',\n",
    "# 'all-MiniLM-L12-v2', 'jhgan/ko-sroberta-sts'\n",
    "]\n",
    "\n",
    "scaler = joblib.load('ko-sroberta-multitask-scaler.pkl')\n",
    "\n",
    "for i in pre_trained_models:\n",
    "#     txt_embed = text_embedding(model_name = i)\n",
    "#     X_embed = txt_embed.transform(features,valid_csv['발화문'])\n",
    "    X_embed = np.load('./X_valid_embed.npy') # 시간 오래 걸리는 작업 그냥 저장한 거 불러오자\n",
    "    y_valid = valid_csv.iloc[:,-7:]\n",
    "   \n",
    "\n",
    "\n",
    "    X_valid_scaled = scaler.transform(X_embed)\n",
    "\n",
    "    X_valid_scaled = np.expand_dims(X_valid_scaled,axis =1)\n",
    "    \n",
    "    test_loss, test_acc = model.evaluate(X_valid_scaled, y_valid, verbose=1)\n",
    "    print(\"Pre-trained Model: \", i)\n",
    "    print(\"Test Accuracy: \",test_acc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44da48ad",
   "metadata": {},
   "source": [
    "### valid 기록이 별로 좋지 않다...\n",
    "\n",
    "- 성능 개선 방안\n",
    "    - 데이터 추가 수집, 학습\n",
    "    - 모델 평가 기준 f1_score로 해서 모델의 학습 상황 점검\n",
    "    - 일단은 f1_score로 해 보자(7차 시도로..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f35d97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
