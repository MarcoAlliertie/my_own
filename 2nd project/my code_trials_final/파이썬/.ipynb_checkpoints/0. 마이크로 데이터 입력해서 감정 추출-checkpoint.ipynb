{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f15e9c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   0 Microsoft Sound Mapper - Input, MME (2 in, 0 out)\n",
       ">  1 머리에 거는 수화기(QCY ArcBuds Hands-Fr, MME (1 in, 0 out)\n",
       "   2 Microsoft Sound Mapper - Output, MME (0 in, 2 out)\n",
       "<  3 헤드폰(QCY ArcBuds Stereo), MME (0 in, 2 out)\n",
       "   4 Digital Audio (S/PDIF)(High Def, MME (0 in, 2 out)\n",
       "   5 머리에 거는 수화기(QCY ArcBuds Hands-Fr, MME (0 in, 1 out)\n",
       "   6 EDGE 24FH3A(NVIDIA High Definit, MME (0 in, 2 out)\n",
       "   7 SPDIF Out (HD Audio SPDIF out), Windows WDM-KS (0 in, 2 out)\n",
       "   8 Output (NVIDIA High Definition Audio), Windows WDM-KS (0 in, 2 out)\n",
       "   9 헤드폰 (), Windows WDM-KS (0 in, 2 out)\n",
       "  10 머리에 거는 수화기 (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free AG Audio%0\n",
       ";(QCY ArcBuds)), Windows WDM-KS (0 in, 1 out)\n",
       "  11 머리에 거는 수화기 (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free AG Audio%0\n",
       ";(QCY ArcBuds)), Windows WDM-KS (1 in, 0 out)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import keras\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sd.query_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6f9da134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start recording\n",
      "stop recording\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smhrd\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\n",
      "당신의 1번째 감정은 Angry일 확률 : 0.00\n",
      "당신의 2번째 감정은 Disgust일 확률 : 0.00\n",
      "당신의 3번째 감정은 Fear일 확률 : 0.00\n",
      "당신의 4번째 감정은 Happiness일 확률 : 0.00\n",
      "당신의 5번째 감정은 Neutral일 확률 : 1.00\n",
      "당신의 6번째 감정은 Sadness일 확률 : 0.00\n",
      "당신의 7번째 감정은 Surprise일 확률 : 0.00\n",
      "난 무서워서 정말 즐거워\n"
     ]
    }
   ],
   "source": [
    "import queue, os, threading\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import time\n",
    "from scipy.io.wavfile import write\n",
    "import speech_recognition as sr\n",
    "\n",
    "q = queue.Queue()\n",
    "recorder = False\n",
    "recording = False\n",
    "\n",
    "def complicated_record():\n",
    "    with sf.SoundFile(\"./micInputTest.wav\", mode='w', samplerate=16000, subtype='PCM_16', channels=1) as file:\n",
    "        with sd.InputStream(samplerate=16000, dtype='int16', channels=1, callback=complicated_save):\n",
    "            while recording:\n",
    "                file.write(q.get())\n",
    "        \n",
    "def complicated_save(indata, frames, time, status):\n",
    "    q.put(indata.copy())\n",
    "    \n",
    "def start():\n",
    "    global recorder\n",
    "    global recording\n",
    "    recording = True\n",
    "    recorder = threading.Thread(target=complicated_record)\n",
    "    print('start recording')\n",
    "    recorder.start()\n",
    "    \n",
    "def stop():\n",
    "    global recorder\n",
    "    global recording\n",
    "    recording = False\n",
    "    recorder.join()\n",
    "    print('stop recording')\n",
    "    \n",
    "start()\n",
    "time.sleep(5)\n",
    "stop()\n",
    "\n",
    "# Initialize the Recognizer\n",
    "r = sr.Recognizer()\n",
    "sentence = \"\"\n",
    "# try :\n",
    "audioFile = sr.AudioFile('./micInputTest.wav')\n",
    "with audioFile as source :\n",
    "    audio = r.record(source)\n",
    "sentence = r.recognize_google(audio, language='ko-KR')\n",
    "get_answer(audioFile.filename_or_fileobject,sentence)\n",
    "\n",
    "# except :\n",
    "#     print(\"no input\")\n",
    "    \n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "800a2855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptual_sharpness(audio_path, sr=16000, n_fft=512, hop_length=256):\n",
    "    # 음원 파일 로드\n",
    "    y= audio_path\n",
    "    sr = sr\n",
    "\n",
    "    # STFT 수행\n",
    "    D = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length))\n",
    "\n",
    "    # 주파수 대역별로 에너지 계산\n",
    "    energy = np.sum(D, axis=0)\n",
    "\n",
    "    # 고주파수 대역 성분 추출\n",
    "    high_freq_energy = energy[3000:8000]  # 예시로 3000Hz에서 6000Hz 사이의 주파수 대역을 고주파수 대역으로 설정\n",
    "\n",
    "    # Perceptual Sharpness 계산\n",
    "    sharpness = np.sum(np.log1p(high_freq_energy))\n",
    "\n",
    "    return sharpness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9bbf96",
   "metadata": {},
   "source": [
    "### 음성 추출 및 벡터화 기능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1b098959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 함수 설정\n",
    "class text_embedding():\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "  \n",
    "    def transform(self, X,raw_label):\n",
    "        embedding_model = SentenceTransformer(self.model_name)\n",
    "        embedding_vec = embedding_model.encode(raw_label)\n",
    "        X_val = np.concatenate((X, embedding_vec))\n",
    "        return X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "27465d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emotion(data):\n",
    "    audio, sr = librosa.load(data, sr=16000)\n",
    "        ##Mel-spectrogram 구현\n",
    "    spectrogram = librosa.stft(audio, n_fft=512, hop_length= 256) \n",
    "    power_spectrogram = spectrogram**2\n",
    "    mel = librosa.feature.melspectrogram(S=power_spectrogram, sr=sr)\n",
    "    mel = librosa.power_to_db(np.abs(mel)**2)\n",
    "    #mfcc 구현\n",
    "    mfccs = librosa.feature.mfcc(S = mel, n_mfcc=20)\n",
    "\n",
    "    stft = np.abs(spectrogram)\n",
    "    chroma_stft = librosa.feature.chroma_stft(S=stft,hop_length=512)\n",
    "    rms = librosa.feature.rms(y=audio)\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
    "    spectral_bandwidths = librosa.feature.spectral_bandwidth(y=audio, sr=sr)\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)\n",
    "    zero_crossing_rates = librosa.feature.zero_crossing_rate(y=audio)\n",
    "    chroma_cens = librosa.feature.chroma_cens(y=audio, sr=sr)\n",
    "    tempo, _ = librosa.beat.beat_track(y=audio, sr=sr)\n",
    "    ps = perceptual_sharpness(audio)\n",
    "\n",
    "\n",
    "    mfccs_mean = mfccs.mean(axis=1)\n",
    "    mfccs_var = mfccs.mean(axis=1)\n",
    "\n",
    "    for k in range(len(mfccs_mean)):\n",
    "        locals()[f'mfccs_mean_{k}'] = mfccs_mean[k]\n",
    "        locals()[f'mfccs_var_{k}'] = mfccs_var[k]\n",
    "    chroma_stft_mean = chroma_stft.mean()\n",
    "    chroma_stft_var = chroma_stft.var()\n",
    "    rms_mean = rms.mean()\n",
    "    rms_var = rms.var()\n",
    "    spectral_centroids_mean = spectral_centroids.mean()\n",
    "    spectral_centroids_var = spectral_centroids.var()\n",
    "    spectral_bandwidths_mean = spectral_bandwidths.mean()\n",
    "    spectral_bandwidths_var = spectral_bandwidths.var()\n",
    "    spectral_rolloff_mean = spectral_rolloff.mean()\n",
    "    spectral_rolloff_var = spectral_rolloff.var()\n",
    "    zero_crossing_rates_mean = zero_crossing_rates.mean()\n",
    "    zero_crossing_rates_var = zero_crossing_rates.var()\n",
    "    harmony_mean = chroma_cens.mean()\n",
    "    harmony_var = chroma_cens.var()\n",
    "    tempo_mean = tempo.mean()\n",
    "    tempo_var = tempo.var()\n",
    "    perceptual_sharpness_mean = ps.mean()\n",
    "    perceptual_sharpness_var = ps.var()\n",
    "\n",
    "\n",
    "        #합치기\n",
    "    features = np.array([])\n",
    "    for j in range(20):\n",
    "        features = np.hstack((features,locals()[f'mfccs_mean_{j}'],locals()[f'mfccs_var_{j}']))\n",
    "\n",
    "    features = np.hstack((features,chroma_stft_mean,chroma_stft_var,rms_mean,rms_var,spectral_centroids_mean\n",
    "                         ,spectral_centroids_var, spectral_bandwidths_mean, spectral_bandwidths_var,spectral_rolloff_mean\n",
    "                         , spectral_rolloff_var,zero_crossing_rates_mean, zero_crossing_rates_var, harmony_mean, harmony_var\n",
    "                         , tempo_mean,tempo_var,perceptual_sharpness_mean,perceptual_sharpness_var))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3322fd80",
   "metadata": {},
   "source": [
    "### 모델과 스케일러 불러와 features에 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c4d1893f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입니다\n"
     ]
    }
   ],
   "source": [
    "def get_answer(file,sentence):\n",
    "    model = keras.models.load_model('./model/model_06-0.8196.keras')\n",
    "    scaler = joblib.load('./scaler.pkl')\n",
    "    \n",
    "    features = extract_emotion(file)\n",
    "    \n",
    "    \n",
    "    txt_embed = text_embedding(model_name = 'jhgan/ko-sroberta-multitask')\n",
    "    X_embed = txt_embed.transform(features,sentence)\n",
    "    \n",
    "    X_embed = np.array(X_embed).reshape(-1,826)\n",
    "    \n",
    "    X_scaled = scaler.transform(X_embed)\n",
    "    X_scaled = np.expand_dims(X_scaled, axis = 1)\n",
    "    pre = model.predict(X_scaled)\n",
    "\n",
    "    emotion_list = ['Angry','Disgust','Fear','Happiness','Neutral','Sadness','Surprise']\n",
    "\n",
    "\n",
    "    # 각 클래스에 속할 확률 출력\n",
    "    for i, prob in enumerate(pre[0]):\n",
    "        print(f\"당신의 {i + 1}번째 감정은 {emotion_list[i]}일 확률 : {prob:.2f}\")\n",
    "print('입니다')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
