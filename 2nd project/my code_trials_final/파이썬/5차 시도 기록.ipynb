{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ba59d2",
   "metadata": {},
   "source": [
    "### 기본적으로는 4차 시도와 동일한 특징으로 가되 이상치 확인을 위해 스케일링은 나중에 미루고 이상치 처리 후 스케일링하기로 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a48987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = train_csv\n",
    "train_data = []\n",
    "file_list = data['wav_id']\n",
    "# file_list = data[0] #테스트용\n",
    "\n",
    "audio_path = train_audio_path\n",
    "# audio_path = test_path #테스트용\n",
    "for i, file_name in tqdm(enumerate(file_list)):\n",
    "\n",
    "    audio, sr = librosa.load(os.path.join(audio_path, f'{file_name}.wav'), sr=16000)\n",
    "        ##Mel-spectrogram 구현\n",
    "    spectrogram = librosa.stft(audio, n_fft=512, hop_length= 256) \n",
    "    power_spectrogram = spectrogram**2\n",
    "    mel = librosa.feature.melspectrogram(S=power_spectrogram, sr=sr)\n",
    "    mel = librosa.power_to_db(np.abs(mel)**2)\n",
    "    #mfcc 구현\n",
    "    mfccs = librosa.feature.mfcc(S = mel, n_mfcc=20)\n",
    "\n",
    "    stft = np.abs(spectrogram)\n",
    "    chroma_stft = librosa.feature.chroma_stft(S=stft,hop_length=512)\n",
    "    rms = librosa.feature.rms(y=audio)\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
    "    spectral_bandwidths = librosa.feature.spectral_bandwidth(y=audio, sr=sr)\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)\n",
    "    zero_crossing_rates = librosa.feature.zero_crossing_rate(y=audio)\n",
    "    chroma_cens = librosa.feature.chroma_cens(y=audio, sr=sr)\n",
    "    tempo, _ = librosa.beat.beat_track(y=audio, sr=sr)\n",
    "    ps = perceptual_sharpness(audio)\n",
    "    \n",
    "    try:\n",
    "        mfccs_mean = mfccs.mean(axis=1)\n",
    "        mfccs_var = mfccs.mean(axis=1)\n",
    "\n",
    "        for k in range(len(mfccs_mean)):\n",
    "            locals()[f'mfccs_mean_{k}'] = mfccs_mean[k]\n",
    "            locals()[f'mfccs_var_{k}'] = mfccs_var[k]\n",
    "        chroma_stft_mean = chroma_stft.mean()\n",
    "        chroma_stft_var = chroma_stft.var()\n",
    "        rms_mean = rms.mean()\n",
    "        rms_var = rms.var()\n",
    "        spectral_centroids_mean = spectral_centroids.mean()\n",
    "        spectral_centroids_var = spectral_centroids.var()\n",
    "        spectral_bandwidths_mean = spectral_bandwidths.mean()\n",
    "        spectral_bandwidths_var = spectral_bandwidths.var()\n",
    "        spectral_rolloff_mean = spectral_rolloff.mean()\n",
    "        spectral_rolloff_var = spectral_rolloff.var()\n",
    "        zero_crossing_rates_mean = zero_crossing_rates.mean()\n",
    "        zero_crossing_rates_var = zero_crossing_rates.var()\n",
    "        harmony_mean = chroma_cens.mean()\n",
    "        harmony_var = chroma_cens.var()\n",
    "        tempo_mean = tempo.mean()\n",
    "        tempo_var = tempo.var()\n",
    "        perceptual_sharpness_mean = ps.mean()\n",
    "        perceptual_sharpness_var = ps.var()\n",
    "    except:\n",
    "        print(f'{i}번째 파일에서 문제 발생')\n",
    "        continue\n",
    "\n",
    "    #합치기\n",
    "    features = np.array([])\n",
    "    for j in range(20):\n",
    "        features = np.hstack((features,locals()[f'mfccs_mean_{j}'],locals()[f'mfccs_var_{j}']))\n",
    "\n",
    "    features = np.hstack((features,chroma_stft_mean,chroma_stft_var,rms_mean,rms_var,spectral_centroids_mean\n",
    "                         ,spectral_centroids_var, spectral_bandwidths_mean, spectral_bandwidths_var,spectral_rolloff_mean\n",
    "                         , spectral_rolloff_var,zero_crossing_rates_mean, zero_crossing_rates_var, harmony_mean, harmony_var\n",
    "                         , tempo_mean,tempo_var,perceptual_sharpness_mean,perceptual_sharpness_var))\n",
    "#     features = scale(features)\n",
    "#     features = extract_features(os.path.join(audio_path, f'{file_name}.wav')) - 일단  스케일링 없이 뽑고 이상치 확인 후 제거\n",
    "    train_data.append(features)\n",
    "    if i>0 and i%100 == 0:\n",
    "        print(f'{i}번째 파일 완료')\n",
    "        locals()[f'X_{int(i/100)}'] = pd.DataFrame(train_data) #i/는 실수형으로 나오니까 i//를 쓰든지 이렇게 int화하든지\n",
    "        \n",
    "        #아마 또 28980쯤에서 오류 날 테니까 해당 파일 뜯어봐서 문제가 뭔지 발견하고 다시 진행한 다음 28900까지 저장된 X_289 DF랑 합치자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f4a0b6",
   "metadata": {},
   "source": [
    "### 검사 결과 특별한 이상치를 발견하지 못함\n",
    "- 스케일링하면 features4와 다를 바가 없으므로 일단 이 자체를 특성으로 저장하고 나중에 각 모델에 적용할 때 스케일링하기로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ceb747",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(train_data).reshape(-1,58)\n",
    "features.shape\n",
    "np.save('./features5',features) #scale되지 않은 1차원 음성 데이터. features4는 추출과정에서 이미 scale됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405d1e27",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d7f98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_history = []\n",
    "y = train['감정']\n",
    "\n",
    "X_train, X_test, y_train,y_test =train_test_split(X,y,random_state=5,stratify=y,test_size=0.3)\n",
    "#MinMAx Scaler을 이용한 특징 벡터 전처리\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#분류기 커널 설정\n",
    "clf = SVC(C=100, kernel='rbf', probability=True)\n",
    "#분류기 학습\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "#예측 결과\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "accuracy_history.append(accuracy_score(y_pred, y_test))\n",
    "    \n",
    "\n",
    "    \n",
    "print(\"정확도 :\", np.mean(accuracy_history))\n",
    "\n",
    "#결과 약 51% 정도의 정확도 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefb42d5",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b63714",
   "metadata": {},
   "outputs": [],
   "source": [
    "y =train.iloc[:,-7:]\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=5,stratify=y,test_size=0.3)\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "scaler =StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(units=256, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=7, activation='softmax'))  # Adjusted to use np.unique for flexibility\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=1, validation_data=(X_test_scaled, y_test),callbacks=[rlrp,mcp,es])\n",
    "\n",
    "# Retrieve training and validation accuracy\n",
    "train_accuracy = history.history['accuracy'][-1]  # Last epoch accuracy\n",
    "val_accuracy = history.history['val_accuracy'][-1]  # Last epoch validation accuracy\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "# 임계값 설정\n",
    "threshold = 0.5\n",
    "\n",
    "# 확률이 임계값보다 크면 1로, 작으면 0으로 분류\n",
    "y_pred_labels = (y_pred > threshold).astype(int)\n",
    "# y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "\n",
    "# Print accuracies\n",
    "print('Training accuracy:', train_accuracy)\n",
    "print('Validation accuracy:', val_accuracy)\n",
    "print('Test accuracy:', test_accuracy)\n",
    "\n",
    "# 약 48% 정도의 정확성을 보임"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
